<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>R5by&#39;s Blog</title>
  
  <subtitle>Welcome</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ruby-.github.io/"/>
  <updated>2020-03-05T04:54:34.000Z</updated>
  <id>https://ruby-.github.io/</id>
  
  <author>
    <name>R5by</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubeedge Examples (Temperature Sensor Demo)</title>
    <link href="https://ruby-.github.io/2020/03/05/kubeedge-temperature-demo/"/>
    <id>https://ruby-.github.io/2020/03/05/kubeedge-temperature-demo/</id>
    <published>2020-03-04T21:45:05.000Z</published>
    <updated>2020-03-05T04:54:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post explain the setup of our kubeedge temperature demo in lab, which can be found in <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2V4YW1wbGVzL3RyZWUvbWFzdGVyL2t1YmVlZGdlLXRlbXBlcmF0dXJlLWRlbW8=" title="https://github.com/kubeedge/examples/tree/master/kubeedge-temperature-demo">here<i class="fa fa-external-link"></i></span>. I’ve spent too much time on this due to the lack of documentation, thus I’ve documented our experience steps in details here just in case someone may find this useful.</p><h2 id="Pre-requisites"><a href="#Pre-requisites" class="headerlink" title="Pre-requisites"></a>Pre-requisites</h2><p>To run this demo, a valid deployment of Kubeedge on K8s cluster is required. If you haven’t met this pre-requisite, please refer to my previous posts on “k3s+kubeedge setups” on deploying k3s (v0.10.2) and kubeedge (v1.0.0). Or, you may stay with me in this post to follow how did we setup kubeedge in release 1.1.0 starting from <a href>here</a>.</p><p>To check whether your kubeedge cluster is functioning in the correct manner, simply do:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $KUBEEDGE=&#123;your-path-to-kubeedge&#125;</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$KUBEEDGE</span>/build</span><br><span class="line"></span><br><span class="line"><span class="comment"># change deployment.yaml to deployment-armv7.yaml if your edge node is on the RPi 3</span></span><br><span class="line">kc apply -f deployment.yaml</span><br></pre></td></tr></table></figure><p>If everything is done correctly, the nginx deployment should be up and running and you may verify the pod’s status by <code>kc get pod</code>. At this point, you may start to follow their documentation on deploying the examples, you’ll probably get stuck on some errors and couldn’t figure where goes wrong, if that’s the case for you please continue reading (o.w. you may close this page and free to leave :D).</p><h2 id="Upgrade-to-Kubeedge-v1-1-0"><a href="#Upgrade-to-Kubeedge-v1-1-0" class="headerlink" title="Upgrade to Kubeedge v1.1.0"></a>Upgrade to Kubeedge v1.1.0</h2><p>At the beginning when I failed following their documentation, I thought that was caused by the incompatibility of the versions (the demo came out several months after they release v1.0.0). So I made a decision to upgrade the kubeedge version from v1.0.0 to v1.1.0. This section states one way in which you may deploy kubeedge v1.1.0 with a valid k3s master in your own environment.</p><blockquote><p>Use <code>kc -n kube-system get pod</code> to get a list of deployed k3s master pods. Refer to my previous posts to see how to disable the modules we do not need. Check the log of your coredns to ensure there is no error message (o.w. flush the iptable and kill these pods twice to solve the issues related to udp connection).</p></blockquote><p>The deployment of cloudcore of kubeedge v1.1.0 is similar to v1.0.0, just remember to change all “edgecontroller” in the yaml files under <code>build/cloud/</code> path to “edgecore”. Also, the creation of device/deviceModel CRDs is no longer an optional in this version, as soon as the cloudcore is up, one should immediately apply these resources before moving on:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create CRDs: devices_v1alpha1_device.yaml &amp; devices_v1alpha1_devicemodel.yaml</span></span><br><span class="line"><span class="comment"># A quick reference can be found here: https://github.com/kubeedge/kubeedge/blob/master/docs/proposals/device-crd.md</span></span><br><span class="line">kc create -f build/crds/devices</span><br></pre></td></tr></table></figure><p>For the edge part, we suggest deploying both edgecore and mqtt broker at bare metal:</p><ul><li><p>1) To bring up a MQTT broker, simply install mosquitto and issue <code>mosquitto -v -p 1883</code> (We suggest to keep the terminal in order to verify the log infos);</p></li><li><p>2) Cross-compile the edgecore and scp it to the RPi 3;</p></li><li><p>3) Copy and modify the <code>edge/conf</code> files to the Rpi 3 to match your own environment (make sure the <code>conf/</code> stays at the same path with the edgecore binary);</p></li><li><p>4) Launch the edgecore from terminal by <code>./edgecore</code> to keep watching the log information.</p></li></ul><p>If you are not sure whether your <code>conf/edge.yaml</code> file is correct, you may refer my settings in below (change wherever I’ve marked in angle brackets):</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mqtt:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="attr">tcp://&lt;mqtt-server-ip&gt;:1883</span> <span class="comment"># external mqtt broker url.</span></span><br><span class="line"><span class="attr">    internal-server:</span> <span class="attr">tcp://127.0.0.1:1884</span> <span class="comment"># internal mqtt broker url.</span></span><br><span class="line"><span class="attr">    mode:</span> <span class="number">2</span> <span class="comment"># 0: internal mqtt broker enable only. 1: internal and external mqtt broker enable. 2: external mqtt broker enable only.</span></span><br><span class="line"><span class="attr">    qos:</span> <span class="number">0</span> <span class="comment"># 0: QOSAtMostOnce, 1: QOSAtLeastOnce, 2: QOSExactlyOnce.</span></span><br><span class="line"><span class="attr">    retain:</span> <span class="literal">false</span> <span class="comment"># if the flag set true, server will store the message and can be delivered to future subscribers.</span></span><br><span class="line"><span class="attr">    session-queue-size:</span> <span class="number">100</span> <span class="comment"># A size of how many sessions will be handled. default to 100.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">edgehub:</span></span><br><span class="line"><span class="attr">    websocket:</span></span><br><span class="line"><span class="attr">        url:</span> <span class="attr">wss://&lt;cloudcore-server-ip&gt;:&lt;port&gt;/e632aba927ea4ac2b575ec1603d56f10/&lt;edge-node-id&gt;/events</span></span><br><span class="line"><span class="attr">        certfile:</span> <span class="string">/etc/kubeedge/certs/edge.crt</span></span><br><span class="line"><span class="attr">        keyfile:</span> <span class="string">/etc/kubeedge/certs/edge.key</span></span><br><span class="line"><span class="attr">        handshake-timeout:</span> <span class="number">30</span> <span class="comment">#second</span></span><br><span class="line"><span class="attr">        write-deadline:</span> <span class="number">15</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">        read-deadline:</span> <span class="number">15</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">    quic:</span></span><br><span class="line"><span class="attr">        url:</span> <span class="string">&lt;cloudcore-server-ip&gt;:10001</span></span><br><span class="line"><span class="attr">        cafile:</span> <span class="string">/etc/kubeedge/ca/rootCA.crt</span></span><br><span class="line"><span class="attr">        certfile:</span> <span class="string">/etc/kubeedge/certs/edge.crt</span></span><br><span class="line"><span class="attr">        keyfile:</span> <span class="string">/etc/kubeedge/certs/edge.key</span></span><br><span class="line"><span class="attr">        handshake-timeout:</span> <span class="number">30</span> <span class="comment">#second</span></span><br><span class="line"><span class="attr">        write-deadline:</span> <span class="number">15</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">        read-deadline:</span> <span class="number">15</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">    controller:</span></span><br><span class="line"><span class="attr">        protocol:</span> <span class="string">websocket</span> <span class="comment"># websocket, quic</span></span><br><span class="line"><span class="attr">        heartbeat:</span> <span class="number">15</span>  <span class="comment"># second</span></span><br><span class="line"><span class="attr">        project-id:</span> <span class="string">e632aba927ea4ac2b575ec1603d56f10</span></span><br><span class="line"><span class="attr">        node-id:</span> <span class="string">&lt;edge-node-id&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">edged:</span></span><br><span class="line"><span class="attr">    register-node-namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">    hostname-override:</span> <span class="string">&lt;edge-node-id&gt;</span> </span><br><span class="line"><span class="attr">    interface-name:</span> <span class="string">&lt;edge-node-net-interface&gt;</span></span><br><span class="line"><span class="attr">    edged-memory-capacity-bytes:</span> <span class="number">7852396000</span></span><br><span class="line"><span class="attr">    node-status-update-frequency:</span> <span class="number">10</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">    device-plugin-enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">    gpu-plugin-enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">    image-gc-high-threshold:</span> <span class="number">80</span> <span class="comment"># percent</span></span><br><span class="line"><span class="attr">    image-gc-low-threshold:</span> <span class="number">40</span> <span class="comment"># percent</span></span><br><span class="line"><span class="attr">    maximum-dead-containers-per-container:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">    docker-address:</span> <span class="attr">unix:///var/run/docker.sock</span></span><br><span class="line"><span class="attr">    runtime-type:</span> <span class="string">docker</span></span><br><span class="line"><span class="attr">    remote-runtime-endpoint:</span> <span class="attr">unix:///var/run/dockershim.sock</span></span><br><span class="line"><span class="attr">    remote-image-endpoint:</span> <span class="attr">unix:///var/run/dockershim.sock</span></span><br><span class="line"><span class="attr">    runtime-request-timeout:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    podsandbox-image:</span> <span class="string">&lt;select-right-one-from-following-comments&gt;</span> <span class="comment"># kubeedge/pause:3.1 for x86 arch , kubeedge/pause-arm:3.1 for arm arch, kubeedge/pause-arm64 for arm64 arch</span></span><br><span class="line"><span class="attr">    image-pull-progress-deadline:</span> <span class="number">60</span> <span class="comment"># second</span></span><br><span class="line"><span class="attr">    cgroup-driver:</span> <span class="string">cgroupfs</span> <span class="comment"># <span class="doctag">NOTE:</span> Need to be consistent with your docker cgroup driver, o.w. the node status will always be "NotReady"</span></span><br><span class="line"><span class="attr">    node-ip:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    cluster-dns:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    cluster-domain:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="attr">mesh:</span></span><br><span class="line"><span class="attr">    loadbalance:</span></span><br><span class="line"><span class="attr">        strategy-name:</span> <span class="string">RoundRobin</span></span><br></pre></td></tr></table></figure><p>If the configuration is correct, you should be able to see some log information about connection between kubeedge cloudcore and edgecore, and edgecore with the mqtt broker. Generally speaking, the edgecore is responsible to get the sensor reading through MQTT subscription, and then push that data upstream to the cloudcore. So next we are about to show how to create/deploy a kubeedge mapper to collect and publish the sensor data.</p><blockquote><p>Don’t forget to create the node resource via <code>kc create -f build/node.json</code> to include this newly started edge node to the cluster. If correct, one should be able to see the node in “Ready” status.</p></blockquote><h2 id="The-Kubeedge-Mapper"><a href="#The-Kubeedge-Mapper" class="headerlink" title="The Kubeedge Mapper"></a>The Kubeedge Mapper</h2><p>The mapper source code for this demo is contained in <code>$GOPATH/src/github.com/kubeedge/examples/kubeedge-temperature-demo/temperature-mapper/main.go</code>. Since the size of this source is quite small, we suggest to obtain its source code and directly build it on your edge node for deployment. However, if you simply need it for deployment, feel free to grab mine in <code>r5by/kubeedge-temperature-mapper:v1.0.0</code> for RPi 3 (o.w. you may use <code>docker build -t &lt;your-image-name&gt; .</code> command to prepare the mapper for your own usage).</p><blockquote><p>Remember, if you use <code>docker build</code> command, your result image can be only deployed on that architecture (e.g. build on RPi 3, only run on arm). </p></blockquote><p>The mapper basically does two things, read sensor data from the pin and publish the readings to the mqtt broker. A detailed explanation of each modules can be found <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2t1YmVlZGdlL2Jsb2IvbWFzdGVyL2RvY3MvbW9kdWxlcy9jbG91ZC9kZXZpY2VfY29udHJvbGxlci5tZA==" title="https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/device_controller.md">here<i class="fa fa-external-link"></i></span>. And the correct way to connect the sensor in the real physical world is also shown in their <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2V4YW1wbGVzL3RyZWUvbWFzdGVyL2t1YmVlZGdlLXRlbXBlcmF0dXJlLWRlbW8=" title="https://github.com/kubeedge/examples/tree/master/kubeedge-temperature-demo">github<i class="fa fa-external-link"></i></span> repo. Do the following on your cloud side once you obtain the source codes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/kubeedge/examples/kubeedge-temperature-demo/crds</span><br><span class="line"></span><br><span class="line">kubectl apply -f devicemodel.yaml</span><br><span class="line">kubectl apply -f device.yaml</span><br></pre></td></tr></table></figure><p><em>NOTE</em>: Here, stop! I need you to verify if kubeedge indeed creates the crd instances by:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Go to the same path where you put your edgecore on your edge node, you should be able to see an `edge.db` file,</span></span><br><span class="line">sqlite3 edge.db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inside the sqlite CLI interface:</span></span><br><span class="line">&gt; .table <span class="comment"># you should see several tables including devices</span></span><br><span class="line">&gt; .header on</span><br><span class="line">&gt; .mode column</span><br><span class="line">&gt; <span class="built_in">read</span> * from devices; <span class="comment"># if nothing is listed here, you fail!</span></span><br><span class="line">&gt; .<span class="built_in">exit</span> <span class="comment"># quit sqlite after verification</span></span><br></pre></td></tr></table></figure><p>Now, what could cause your troubles here are:</p><ul><li>1) You misconfigured your crd instances. Double check your <code>device.yaml</code> file, it’s like this:</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">devices.kubeedge.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Device</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">temperature</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    description:</span> <span class="string">'temperature'</span></span><br><span class="line"><span class="attr">    manufacturer:</span> <span class="string">'test'</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  deviceModelRef:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">temperature-model</span></span><br><span class="line"><span class="attr">  nodeSelector:</span></span><br><span class="line"><span class="attr">    nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">      - matchExpressions:</span></span><br><span class="line"><span class="attr">          - key:</span> <span class="string">''</span></span><br><span class="line"><span class="attr">            operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">            values:</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">&lt;your-node-id&gt;</span> <span class="comment"># NOTE here, this should be your node id other than its label!!</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  twins:</span></span><br><span class="line"><span class="attr">    - propertyName:</span> <span class="string">temperature-status</span></span><br><span class="line"><span class="attr">      desired:</span></span><br><span class="line"><span class="attr">        metadata:</span></span><br><span class="line"><span class="attr">          type:</span> <span class="string">string</span></span><br><span class="line"><span class="attr">        value:</span> <span class="string">''</span></span><br></pre></td></tr></table></figure><ul><li>2) You made some changes according to 1) but still it’s not there. This is because the kubeedge doesn’t re-apply your changes if you simply do <code>kc apply -f crds</code>. You will need to first delete these crds then re-create them to enable the writing into this database.</li></ul><p>Once you solve the above problems, nothing shall bother you any more. Simple create this mapper and follow the rest of official documentation to read your sensor from upstream:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/kubeedge/examples/kubeedge-temperature-demo/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Please enter the following details in the deployment.yaml :-</span></span><br><span class="line"><span class="comment">#    1. ~Replace &lt;edge_node_name&gt; with the name of your edge node at spec.template.spec.nodeSelector.name~</span></span><br><span class="line"><span class="comment">#       <span class="doctag">NOTE:</span> &lt;edge_node_name&gt; here actually means the node's label, not its name!!</span></span><br><span class="line"><span class="comment">#    2. Replace &lt;your_image&gt; at spec.template.spec.containers.image</span></span><br><span class="line"></span><br><span class="line">kc create -f deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># The mapper will report back the temperature to cloud after updating. Observe the temperature in the cloud side.</span></span><br><span class="line">kc get device temperature -oyaml -w</span><br></pre></td></tr></table></figure><h2 id="Pitfalls-坑-amp-Trouble-Shooting"><a href="#Pitfalls-坑-amp-Trouble-Shooting" class="headerlink" title="Pitfalls(坑) &amp; Trouble Shooting"></a>Pitfalls(坑) &amp; Trouble Shooting</h2><p>You may or may not come across the following problems, and I put here my solutions for your references:</p><ul><li>Q1: Cross-compile failed with errors: “xxx version: does not match version-control timestamp xxx”</li></ul><blockquote><p>A1: Disable go mod solves this, that is, before your build, do <code>export GO111MODULE=off</code>.</p></blockquote><ul><li>Q2: Cross-compile failed on CentOS 7: “ xxx arm-linux-gnueabi-gcc xxx”</li></ul><blockquote><p>A2: CentOS doesn’t support gnu gcc cross compiler well, simple switch to Ubuntu 18.04 solves this issue for me (Trying to use CentOS’s gun cross compiler doesn’t work for me)</p></blockquote><ul><li>Q3: Where to find the Kubeedge documentation to my target version?</li></ul><blockquote><p>A3: The official documentation is a mess found <span class="exturl" data-url="aHR0cHM6Ly9yZWFkdGhlZG9jcy5vcmcvcHJvamVjdHMva3ViZWVkZ2UvZG93bmxvYWRzL3BkZi9sYXRlc3Qv" title="https://readthedocs.org/projects/kubeedge/downloads/pdf/latest/">here<i class="fa fa-external-link"></i></span>. However the references found under <code>doc</code> source folder are closer to the truth…</p></blockquote><ul><li>Q4: Any Kubeedge MQTT references?</li></ul><blockquote><p>A4: <span class="exturl" data-url="aHR0cHM6Ly9rdWJlZWRnZS5yZWFkdGhlZG9jcy5pby9lbi92MS4wLjAvZ3VpZGVzL21lc3NhZ2VfdG9waWNzLmh0bWw=" title="https://kubeedge.readthedocs.io/en/v1.0.0/guides/message_topics.html">Here<i class="fa fa-external-link"></i></span></p></blockquote><ul><li>Q5: Forgot to delete the mapper deployment now it’s always automatically brought up by k3s, what shall I do?</li></ul><blockquote><p>A5: The “etcd” is replaced by sqlite (in default) for k3s. You may manually delete those registered resources in <code>/var/lib/rancher/k3s/server/db/state.db</code> if can’t delete them from <code>kubectl</code> command.</p></blockquote><ul><li>Q6: Give me a quick Sqlite Manual</li></ul><blockquote><p>A6: <span class="exturl" data-url="aHR0cHM6Ly93d3cucnVub29iLmNvbS9zcWxpdGUvc3FsaXRlLXR1dG9yaWFsLmh0bWw=" title="https://www.runoob.com/sqlite/sqlite-tutorial.html">Here<i class="fa fa-external-link"></i></span></p></blockquote><ul><li>Q7: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</li></ul><blockquote><p>A7: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RvY2tlci9mb3ItbGludXgvaXNzdWVzLzUzNQ==" title="https://github.com/docker/for-linux/issues/535">Refer to this thread<i class="fa fa-external-link"></i></span>; solve by rm the docker related files and restart docker.</p></blockquote><ul><li>Q8: How to check whether MQTT broker can be reached from within another container?</li></ul><blockquote><p>A8: login to the other container and use <code>telnet &lt;mqtt-ip-addr&gt; &lt;mqtt-port&gt;</code> command.</p></blockquote><ul><li>Q9: Understand MQTT server log info.</li></ul><blockquote><p>A9: <span class="exturl" data-url="aHR0cHM6Ly9iaXRlZW5pdS5naXRodWIuaW8vbXF0dC9tb3NxdWl0dG8tbG9nLWFuYWx5c2lzLw==" title="https://biteeniu.github.io/mqtt/mosquitto-log-analysis/">Reference here<i class="fa fa-external-link"></i></span></p></blockquote><ul><li>Q10: What is the <code>qemu-user-static</code> that Kubeedge project uses for cross-compile from within docker?</li></ul><blockquote><p>A10: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL211bHRpYXJjaC9xZW11LXVzZXItc3RhdGljI3FlbXUtdXNlci1zdGF0aWM=" title="https://github.com/multiarch/qemu-user-static#qemu-user-static">Here<i class="fa fa-external-link"></i></span></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post explain the setup of our kubeedge temperature demo in lab, which can be found in &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9naXR
      
    
    </summary>
    
    
      <category term="edge computing, cloud computing" scheme="https://ruby-.github.io/categories/edge-computing-cloud-computing/"/>
    
    
      <category term="kubeedge, k3s" scheme="https://ruby-.github.io/tags/kubeedge-k3s/"/>
    
  </entry>
  
  <entry>
    <title>k3s+kubeedge (3) Deploy Edge Core on Raspberry Pi 3</title>
    <link href="https://ruby-.github.io/2020/02/02/kubeedge-on-k3s-3/"/>
    <id>https://ruby-.github.io/2020/02/02/kubeedge-on-k3s-3/</id>
    <published>2020-02-01T21:37:32.000Z</published>
    <updated>2020-02-01T23:13:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post finalize the setup of Kubeedge on K3S cluster. The edge part of the Kubeedge connect with the API server through CloudHub in the cloud core (i.e. “edgecontroller”). We will deploy the edgecore on two Raspberry Pi 3 nodes.</p><h2 id="Step-1-Check-the-current-environment"><a href="#Step-1-Check-the-current-environment" class="headerlink" title="Step 1. Check the current environment"></a>Step 1. Check the current environment</h2><p>Before you follow the rest of this post, please make sure you have your k3s master and kubeedge edgecontroller service up &amp; running:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input: On master node</span></span><br><span class="line"><span class="comment">## Verify the k3s master</span></span><br><span class="line">kc get node</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># aces-diamonds-ace.localdomain   Ready      master   116d   v1.16.2-k3s.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Verify the edgecontroller</span></span><br><span class="line">kce get svc</span><br><span class="line"><span class="comment"># Output: </span></span><br><span class="line"><span class="comment"># NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE</span></span><br><span class="line"><span class="comment"># edgecontroller   NodePort   10.43.217.231   &lt;none&gt;        10000:30267/TCP,2345:32345/TCP   38m</span></span><br></pre></td></tr></table></figure><p>SSH to your Raspberry Pi and stop the k3s workers (if you have them running at those edge nodes):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input: On Rasp Pi</span></span><br><span class="line">./k3s-killall.sh</span><br></pre></td></tr></table></figure><h2 id="Step-2-Cross-compile-evil-or-good"><a href="#Step-2-Cross-compile-evil-or-good" class="headerlink" title="Step 2. Cross-compile: evil or good"></a>Step 2. Cross-compile: evil or good</h2><p>The next step is to build and save edgecore image. This part is kinda messy for the Kubeedge project (吐了个槽). I’ve tried to follow the guide in their official documentation <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmt1YmVlZGdlLmlvL2VuL2xhdGVzdC9zZXR1cC9zZXR1cC5odG1sI2RlcGxveS10aGUtZWRnZS1ub2Rl" title="https://docs.kubeedge.io/en/latest/setup/setup.html#deploy-the-edge-node">here<i class="fa fa-external-link"></i></span> but failed. If you also confront so many problems like I do when following the official document, I suggest to try the solution as I suggest in below.</p><p>Cross-compile is a feature offered by Kubeedge project out-of-box. However, it doesn’t work as it’s supposed to, at least for my case. First, let’s take a quick look at its README file and I’ll explain what should happen:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> build/edge</span><br><span class="line">vi README.md</span><br></pre></td></tr></table></figure><p>This README file is probably the second worst README instructions you can possibly find all over the github (the worst stays in my own repo). After reading it, little we know about its usefulness. In fact, the script makes use of the docker-compose on setting up both build and deployment environment in mixture, even though it still provides a <code>only_run_edge</code> option, the README instruction mentions nothing about it. After so many trouble shootings, I give up on using the original docker-compose method and adopt the following approach.</p><p>First, since I don’t want to use Rasp Pi to build the project, I use the following commands to cross build the armv7 docker image within my x86_64 master node. The attached <code>run_daemon.sh</code> script provides a way of using <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL211bHRpYXJjaC9xZW11LXVzZXItc3RhdGljI3FlbXUtdXNlci1zdGF0aWM=" title="https://github.com/multiarch/qemu-user-static#qemu-user-static">qemu<i class="fa fa-external-link"></i></span> to achieve such goal. Basically what it does is to simulate a arm-based Docker (but in fact running on a amd host) to build your Dockerfile into images for arm. You probably will confront problems mentioned in <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvbGFuZy9nby9pc3N1ZXMvMTUwMzg=" title="https://github.com/golang/go/issues/15038">#15038<i class="fa fa-external-link"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2t1YmVlZGdlL2lzc3Vlcy8xMDY4" title="https://github.com/kubeedge/kubeedge/issues/1068">#1068<i class="fa fa-external-link"></i></span> and <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RvY2tlci9jb21wb3NlL2lzc3Vlcy83MTYw" title="https://github.com/docker/compose/issues/7160">#7160<i class="fa fa-external-link"></i></span> if you intend to build directly from Rasp Pi. But before you jump to those discussions, you can try to use my modified Dockerfile to replace the original one found under path <code>build/edge/</code> in order to save some time:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ARG</span> BUILD_FROM=golang:<span class="number">1.12</span>-alpine3.<span class="number">10</span></span><br><span class="line"><span class="keyword">ARG</span> RUN_FROM=docker:dind</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> $&#123;BUILD_FROM&#125; AS builder</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> QEMU_ARCH=x86_64</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./build/edge/tmp/qemu-<span class="variable">$&#123;QEMU_ARCH&#125;</span>-static /usr/bin/</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /go/src/github.com/kubeedge/kubeedge</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk --no-cache update &amp;&amp; \</span></span><br><span class="line"><span class="bash">apk --no-cache upgrade &amp;&amp; \</span></span><br><span class="line"><span class="bash">apk add libc-dev &amp;&amp; \</span></span><br><span class="line"><span class="bash">apk add binutils-gold &amp;&amp; \</span></span><br><span class="line"><span class="bash">apk --no-cache add build-base linux-headers sqlite-dev &amp;&amp; \</span></span><br><span class="line"><span class="bash">CGO_ENABLED=1 go build -v -o /usr/<span class="built_in">local</span>/bin/edge_core -ldflags=<span class="string">"-w -s -extldflags -static"</span> \</span></span><br><span class="line"><span class="bash">/go/src/github.com/kubeedge/kubeedge/edge/cmd</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> $&#123;RUN_FROM&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"zhanghongtong &lt;zhanghongtong@foxmail.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /usr/bin/qemu* /usr/bin/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> GOARCHAIUS_CONFIG_PATH /etc/kubeedge/edge</span><br><span class="line"><span class="keyword">ENV</span> database.source /var/lib/kubeedge/edge.db</span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> [<span class="string">"/etc/kubeedge/certs"</span>, <span class="string">"/var/lib/edged"</span>, <span class="string">"/var/lib/kubeedge"</span>, <span class="string">"/var/run/docker.sock"</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /usr/<span class="built_in">local</span>/bin/edge_core /usr/<span class="built_in">local</span>/bin/edge_core</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /go/src/github.com/kubeedge/kubeedge/edge/conf /etc/kubeedge/edge/conf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"edge_core"</span>]</span></span><br></pre></td></tr></table></figure><blockquote><p>The magic button to notice is <code>apk add binutils-gold</code>, refer to the discussions <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS9ndmlzb3IvaXNzdWVzLzI2" title="https://github.com/google/gvisor/issues/26">here<i class="fa fa-external-link"></i></span> for more details.</p></blockquote><p>With the Dockerfile modified, issue the following command to build edgecore for your own arm hosts. Or, if you are also using Raspberry Pi 3, feel free to grab my pre-built images from here <code>r5by/kubeedge_edgecore_armv7:v1.0.0</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build the edgecore for Rasp Pi 3 (arm v7)</span></span><br><span class="line"><span class="built_in">cd</span> build/edge</span><br><span class="line">./run_daemon.sh <span class="built_in">set</span> arch=arm32v7 qemu_arch=arm</span><br><span class="line">./run_daemon.sh build</span><br></pre></td></tr></table></figure><blockquote><p><strong>NOTE</strong>: If you have a Rasp Pi 4 or later, you may need target arm v8. Use <code>./run_daemon.sh set arch=arm64v8 qemu_arch=aarch64</code> instead. If these parameters are not set, by default it builds image for x86_64. The configured parameters will be then written into <code>.env</code> file under the path.</p></blockquote><blockquote><p>If you are not sure whether your build image is indeed for amd or arm, simply use <code>docker inspect &lt;image_id&gt;</code> to check its architecture. Also, if you launch the images on docker with wrong architecture, you will see some error messages like “standard_init_linux.go:xxx: exec user process caused “exec format error””, etc.</p></blockquote><h2 id="Step-3-Launch-edgecore"><a href="#Step-3-Launch-edgecore" class="headerlink" title="Step 3. Launch edgecore"></a>Step 3. Launch edgecore</h2><p>After the edgecore image is prepared, we can now launch the edgecore from the Rasp Pi. Firstly, the certificates and configuration files need to be also available from the edge nodes.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On master:</span></span><br><span class="line">tar czvf kubecert.tar /etc/kubeedge/</span><br><span class="line">scp kubecert.tar &lt;your_pi_node&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># On worker (i.e. Pi):</span></span><br><span class="line"><span class="built_in">cd</span> /</span><br><span class="line">tar zxvf kubecert.tar</span><br></pre></td></tr></table></figure><p>Remember to copy the <code>run_daemon.sh</code> file as well. Then launch the edgecore with the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./run_daemon.sh only_run_edge cloudhub=&lt;your_cloud_hub_ip&gt;:&lt;port&gt; edgename=edge-node-pi-01 image=<span class="string">"r5by/kubeedge_edgecore_armv7:v1.0.0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify your</span></span><br></pre></td></tr></table></figure><blockquote><p>Obtain the <port> number from master node via command <code>k3s kubectl get svc -n kubeedge</code> as we introduced in the previous post.</port></p></blockquote><p>If you have built your edgecore in the correct architecture and everything works, you should be able to see it’s running within docker at your edge. Switch back to your cloud master and edit a new <code>node.yaml</code> file to let your master detect this newly added edge node.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd build/edge; vi node.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  name: edge-node-pi-01</span><br><span class="line">  labels:</span><br><span class="line">    name: edge-node-pi-01</span><br><span class="line">    node-role.kubernetes.io/edge: <span class="string">""</span></span><br></pre></td></tr></table></figure><p>Save the yaml file and apply with command <code>kc apply -f node.yaml</code>. Deploy the edgecore on the second Rasp Pi node similarly, and if you have done everything correctly, you should be able to see your k3s+kubeedge cluster up and running:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kc get node</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># aces-diamonds-ace.localdomain   Ready      master   116d   v1.16.2-k3s.1</span></span><br><span class="line"><span class="comment"># edge-node-pi-01                 Ready      edge     20m    v1.10.9-kubeedge-v1.0.0</span></span><br><span class="line"><span class="comment"># edge-node-pi-02                 Ready      edge     4s     v1.10.9-kubeedge-v1.0.0</span></span><br></pre></td></tr></table></figure><h2 id="Step-4-Summery"><a href="#Step-4-Summery" class="headerlink" title="Step 4. Summery"></a>Step 4. Summery</h2><p>In this series of post, we have shown how to deploy Kubeedge on k3s. In the next post, I’ll jump into some interesting examples provided by Kubeedge open source project to explore its wide usages. Please leave in the comments below to let me know if you may have any trouble when following my tutorials on deploying k3s+kubeedge in your own use case. Peace!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post finalize the setup of Kubeedge on K3S cluster. The edge part of the Kubeedge connect with the API server through CloudHub in th
      
    
    </summary>
    
    
      <category term="edge computing, cloud computing" scheme="https://ruby-.github.io/categories/edge-computing-cloud-computing/"/>
    
    
      <category term="kubeedge, k3s" scheme="https://ruby-.github.io/tags/kubeedge-k3s/"/>
    
  </entry>
  
  <entry>
    <title>k3s+kubeedge (2) Deploy the Cloud Core (edgecontroller)</title>
    <link href="https://ruby-.github.io/2020/02/02/kubeedge-on-k3s-2/"/>
    <id>https://ruby-.github.io/2020/02/02/kubeedge-on-k3s-2/</id>
    <published>2020-02-01T20:03:15.000Z</published>
    <updated>2020-02-01T23:13:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>In the previous post, we know how to set up a code-review/debugging environment (some preparations are actually included in this post though). Now, let’s try to bring up the Kubeedge cloudcore (“edgecontroller” as for v1.0.0) on our k3s master. Again, this post assumes you already have a minimal k3s/k8s master up and running at the server, if you don’t know how to do so, feel free to visit my previous posts on guiding you through the k3s setup.</p><h2 id="Step-1-Preparation-–-k3s-revisit"><a href="#Step-1-Preparation-–-k3s-revisit" class="headerlink" title="Step 1. Preparation – k3s revisit"></a>Step 1. Preparation – k3s revisit</h2><p>In my last post, I forgot to mention about my k3s environment (打脸). Since we are using Docker other than Containerd for k3s, I recommend to change the k3s configuration as following (and disable several services that we are not currently using):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/systemd/system/k3s.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Lightweight Kubernetes</span><br><span class="line">Documentation=https://k3s.io</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/etc/systemd/system/k3s.service.env</span><br><span class="line">ExecStartPre=-/sbin/modprobe br_netfilter</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/k3s \</span><br><span class="line">    server \</span><br><span class="line">    --no-deploy traefik \</span><br><span class="line">    --docker \</span><br><span class="line">    --no-deploy servicelb \</span><br><span class="line"></span><br><span class="line">KillMode=process</span><br><span class="line">Delegate=yes</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5s</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Save the file and reload daemon then restart k3s service:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart k3s</span><br><span class="line"></span><br><span class="line"><span class="comment">#Check the k3s is successfully restarted with the modified configurations</span></span><br><span class="line">systemctl status k3s</span><br></pre></td></tr></table></figure><blockquote><p> Traefik is always restarting automatically with k3s. Thus to truly disable it, one should follow the commands mentioned in <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNzL2lzc3Vlcy83MTcjaXNzdWVjb21tZW50LTU0MTg3NjU4OA==" title="https://github.com/rancher/k3s/issues/717#issuecomment-541876588">here<i class="fa fa-external-link"></i></span>.</p></blockquote><h2 id="Step-2-Build-amp-Deploy-cloudimage"><a href="#Step-2-Build-amp-Deploy-cloudimage" class="headerlink" title="Step 2. Build &amp; Deploy cloudimage"></a>Step 2. Build &amp; Deploy cloudimage</h2><p>Feel free to refresh your memory on how to build the Kubeedge cloudimage in my previous post. However if you only wish to test deploy, you may use mine here <code>r5by/kubeedge_edgecontroller:v1.0.0</code>. After the cloudimage is prepared, the next step is to generate certification files:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> build/cloud</span><br><span class="line">../tools/certgen.sh buildSecret | tee ./06-secret.yaml</span><br></pre></td></tr></table></figure><p>This scripts will generate a bunch of ca/crt files under path <code>/etc/kubeedge/</code> and write the secrets to <code>06-secret.yaml</code>. Then next, we should copy and paste the kubeconfig to that location for later usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/kubeedge/cloud</span><br><span class="line"><span class="comment"># k3s kubeconfig location</span></span><br><span class="line">cp /etc/rancher/k3s/k3s.yaml /etc/kubeedge/cloud/kubeconfig.yaml</span><br></pre></td></tr></table></figure><h2 id="Step-3-Modify-the-other-yaml-files"><a href="#Step-3-Modify-the-other-yaml-files" class="headerlink" title="Step 3. Modify the other yaml files"></a>Step 3. Modify the other yaml files</h2><p>In the <code>build/cloud</code> path, several yaml files are listed:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> build/cloud/; ls</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># 01-namespace.yaml          03-clusterrole.yaml        05-configmap.yaml          07-deployment.yaml         Dockerfile                  README_zh.md</span></span><br><span class="line"><span class="comment"># 02-serviceaccount.yaml     04-clusterrolebinding.yaml 06-secret.yaml             08-service.yaml.example    README.md</span></span><br></pre></td></tr></table></figure><p>Change <code>03-clusterrole.yaml</code> as following:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">    kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">nodes</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">nodes/status</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">configmaps</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">pods</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">pods/status</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">secrets</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">services</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">endpoints</span></span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">get</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">list</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">watch</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">update</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">pods</span></span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">delete</span></span><br></pre></td></tr></table></figure><p>Change <code>05-configmap.yaml</code> as following:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">    kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">controller.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string"></span><span class="attr">    controller:</span></span><br><span class="line"><span class="attr">      kube:</span></span><br><span class="line"><span class="attr">        master:</span> <span class="attr">https://kubernetes.default.svc.cluster.local:443</span></span><br><span class="line"><span class="attr">        kubeconfig:</span> <span class="string">/etc/kubeedge/cloud/kubeconfig.yaml</span></span><br><span class="line"><span class="attr">        namespace:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">        content_type:</span> <span class="string">"application/vnd.kubernetes.protobuf"</span></span><br><span class="line"><span class="attr">        qps:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">        burst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">        node_update_frequency:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">    cloudhub:</span></span><br><span class="line"><span class="attr">      address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">      ca:</span> <span class="string">/etc/kubeedge/certs/rootCA.crt</span></span><br><span class="line"><span class="attr">      cert:</span> <span class="string">/etc/kubeedge/certs/edge.crt</span></span><br><span class="line"><span class="attr">      key:</span> <span class="string">/etc/kubeedge/certs/edge.key</span></span><br><span class="line"><span class="attr">      keepalive-interval:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">      write-timeout:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">      node-limit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">    devicecontroller:</span></span><br><span class="line"><span class="attr">       kube:</span></span><br><span class="line"><span class="attr">         master:</span> <span class="attr">https://kubernetes.default.svc.cluster.local:443</span></span><br><span class="line"><span class="attr">         namespace:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">         content_type:</span> <span class="string">"application/vnd.kubernetes.protobuf"</span></span><br><span class="line"><span class="attr">         qps:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">         burst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">         kubeconfig:</span> <span class="string">/etc/kubeedge/cloud/kubeconfig.yaml</span></span><br><span class="line">           <span class="string">logging.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string"></span><span class="attr">    loggerLevel:</span> <span class="string">"INFO"</span></span><br><span class="line"><span class="attr">    enableRsyslog:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">    logFormatText:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    writers:</span> <span class="string">[stdout]</span></span><br><span class="line">  <span class="string">modules.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string"></span><span class="attr">    modules:</span></span><br><span class="line"><span class="attr">      enabled:</span> <span class="string">[controller,</span> <span class="string">cloudhub]</span></span><br></pre></td></tr></table></figure><p>Then verify if your <code>06-secret.yaml</code> is consistent with the ca/cert files in path <code>/etc/kubeedge/</code>, also copy and paste the <code>07-deployment.yaml</code> and <code>08-service.yaml</code> files from our last post. </p><p>The configurations are now set. Use the shell script we write in last post to initialize start Kubeedge edgecontroller on the cloud.</p><p>If everything works as expect, you should be able to verify the edgecontroller service is up:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `kce` alias `k3s kubectl -n kubeedge`</span></span><br><span class="line">kce get pod</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line"><span class="comment">#NAME                            READY   STATUS    RESTARTS   AGE</span></span><br><span class="line"><span class="comment">#edgecontroller-5dc9955c-p2c7s   1/1     Running   0          5s</span></span><br><span class="line"></span><br><span class="line">kce get svc</span><br><span class="line"><span class="comment">#NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE</span></span><br><span class="line"><span class="comment">#edgecontroller   NodePort   10.43.217.231   &lt;none&gt;        10000:30267/TCP,2345:32345/TCP   27s</span></span><br><span class="line"></span><br><span class="line">kce logs edgecontroller-5dc9955c-p2c7s</span><br><span class="line"><span class="comment"># No error messages should be found...</span></span><br></pre></td></tr></table></figure><h2 id="Step-4-Trouble-Shooting"><a href="#Step-4-Trouble-Shooting" class="headerlink" title="Step 4. Trouble Shooting"></a>Step 4. Trouble Shooting</h2><p>Okay, I understand life is complicated. Unfortunately if you were caught by some weird error messages, don’t panic (even though the compiler does). Check the issues I mentioned in the previous post, and verify carefully each of these yaml files to make sure you understand what should be there. I also listed here several forum posts that may help with your issues:</p><ul><li>1) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVybmV0ZXMvZG5zL2lzc3Vlcy8xMTA=" title="https://github.com/kubernetes/dns/issues/110">DNS issue, not the fix for mine<i class="fa fa-external-link"></i></span></li><li>2) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVybmV0ZXMva3ViZXJuZXRlcy9pc3N1ZXMvNjY0MzI=" title="https://github.com/kubernetes/kubernetes/issues/66432">DNS issue, again not the fix for mine<i class="fa fa-external-link"></i></span></li><li>3) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVybmV0ZXMvbWluaWt1YmUvaXNzdWVzLzQzNTA=" title="https://github.com/kubernetes/minikube/issues/4350">DNS issue, no luck either<i class="fa fa-external-link"></i></span></li><li>4) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVybmV0ZXMva3ViZWFkbS9pc3N1ZXMvMTkzI2lzc3VlY29tbWVudC0zMzAwNjA4NDg=" title="https://github.com/kubernetes/kubeadm/issues/193#issuecomment-330060848">DNS issue, finally works, fix mentioned in here<i class="fa fa-external-link"></i></span></li><li>5) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvcmFuY2hlci9pc3N1ZXMvNjEzOQ==" title="https://github.com/rancher/rancher/issues/6139">DNS issue, not works for mine<i class="fa fa-external-link"></i></span></li><li>6) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNzL2lzc3Vlcy8yNA==" title="https://github.com/rancher/k3s/issues/24">DNS issue, from k3s forum, not a solution for my case<i class="fa fa-external-link"></i></span></li></ul><p>Finally, if you have other questions/problems and solutions, please leave your comments below to share with others. In the next post, I’ll complete the Kubeedge cluster setup with the edgecore setup on both lovely Raspberry Pi’s.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In the previous post, we know how to set up a code-review/debugging environment (some preparations are actually included in this post tho
      
    
    </summary>
    
    
      <category term="edge computing, cloud computing" scheme="https://ruby-.github.io/categories/edge-computing-cloud-computing/"/>
    
    
      <category term="kubeedge, k3s" scheme="https://ruby-.github.io/tags/kubeedge-k3s/"/>
    
  </entry>
  
  <entry>
    <title>k3s+kubeedge (1) Code Review/Debugging Environment Setup</title>
    <link href="https://ruby-.github.io/2020/02/01/kubeedge-on-k3s-1/"/>
    <id>https://ruby-.github.io/2020/02/01/kubeedge-on-k3s-1/</id>
    <published>2020-02-01T03:25:25.000Z</published>
    <updated>2020-02-01T23:13:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2t1YmVlZGdl" title="https://github.com/kubeedge/kubeedge">Kubeedge<i class="fa fa-external-link"></i></span> is a CNCF open source project that aims at extending the existing orchestration system (Kubernetes)’s container management capability to the edge. It provides core infrastructure support for networking, application deployment and metadata synchronization between cloud and edge along with the Kubernetes project. In this series of posts, we’ll see how to deploy Kubeedge on our existing k3s cluster and set up a code review environment where we may set up breakpoint and step-by-step inspect its functionality. This post focuses on the latter part.</p><p>For the purpose of reading code and more importantly, understanding and fixing the errors, it’s better to have a debugging environment set up first. As for our deployment, we currently have one PC server as the master host to deploy the cloud part, and two Raspberry Pi 3 as the worker nodes to deploy the edge part. The main components of Kubeedge’s cloud/edge parts can be viewed from the structure pictured in below:</p><p><img src="https://github.com/kubeedge/kubeedge/raw/master/docs/images/kubeedge_arch.png" alt="image"></p><p>This post is going to demonstrate how to debug the “cloud core” from within the container at deployment. For the edge core, the procedures are similar. You may also build &amp; debug the code from the code directly, but in my case I personally like to have development environment to be close to my deployment (a.k.a in a docker/container execution environment).</p><blockquote><p>Note: In the previous releases (&lt; v1.0.0) of Kubeedge, the “cloud_core” was named as “edgecontroller” in the code, therefore we will use “edgecontroller” to refer to the “cloud core” in this post because we adopt Kubeedge v1.0.0</p></blockquote><blockquote><p>The following steps assumes your already have a successfully deployed &amp; running k3s cluster (or k8s cluster). All though the master (API server) is required, the worker nodes are not.</p></blockquote><h2 id="Step-1-Preparation"><a href="#Step-1-Preparation" class="headerlink" title="Step 1. Preparation"></a>Step 1. Preparation</h2><p>First download the source to the master PC, and checkout the version 1.0.0 for later usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$GOPATH</span>/src/github.com/kubeedge</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/kubeedge</span><br><span class="line">git <span class="built_in">clone</span> git@github.com:kubeedge/kubeedge.git</span><br><span class="line"><span class="comment"># If you only want to compile quickly without using go mod, please set GO111MODULE=off (e.g. export GO111MODULE=off) </span></span><br><span class="line"><span class="built_in">cd</span> kubeedge </span><br><span class="line"></span><br><span class="line"><span class="comment"># Check out version 1.0.0</span></span><br><span class="line">git checkout v1.0.0 -b dev-v1.0.0</span><br></pre></td></tr></table></figure><blockquote><p>I’ve found many issues building the code above version 1.0.0, please feel free to test on your own environment and let me know if you may succeed.</p></blockquote><h2 id="Step-2-Try-to-build-the-cloud-part"><a href="#Step-2-Try-to-build-the-cloud-part" class="headerlink" title="Step 2. Try to build the cloud part"></a>Step 2. Try to build the cloud part</h2><p>Following the instructions from the Kubernetes official document, let’s try to build the cloud image.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make cloudimage</span><br></pre></td></tr></table></figure><p>If everything is working as expected, you will get some message like this:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Successfully built 5f31402ab1ee</span><br><span class="line">Successfully tagged kubeedge/edgecontroller:v1.0.0</span><br></pre></td></tr></table></figure><p>However, if you take that image to deploy on the cloud server, you may get lots of trouble. In my next post, I’ll lead you through several pitfalls I encountered when I was trying to deploy it, but for now, let’s continue on our investigation on how to connect the running container to our IDE for debugging/code review.</p><h2 id="Step-3-Rebuild-the-cloudimage"><a href="#Step-3-Rebuild-the-cloudimage" class="headerlink" title="Step 3. Rebuild the cloudimage"></a>Step 3. Rebuild the cloudimage</h2><p>For the development and code review, I continue to use GoLand IDE (2019.3). The following debugging strategy is mainly inspired by this <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmpldGJyYWlucy5jb20vZ28vMjAxOC8wNC8zMC9kZWJ1Z2dpbmctY29udGFpbmVyaXplZC1nby1hcHBsaWNhdGlvbnMv" title="https://blog.jetbrains.com/go/2018/04/30/debugging-containerized-go-applications/">post<i class="fa fa-external-link"></i></span>.</p><p>First, locate the Dockerfile in the <code>build/cloud</code> path, and replace its content with following:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> golang:<span class="number">1.12</span>.<span class="number">1</span>-alpine3.<span class="number">9</span> AS builder</span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /go/src/github.com/kubeedge/kubeedge</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># RUN CGO_ENABLED=0 go build -v -o /usr/local/bin/edgecontroller -ldflags="-w -s" \</span></span><br><span class="line"><span class="comment"># github.com/kubeedge/kubeedge/cloud/cmd</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> CGO_ENABLED=0 go build -gcflags <span class="string">"all=-N -l"</span> -v -o /usr/<span class="built_in">local</span>/bin/edgecontroller  \</span></span><br><span class="line"><span class="bash">github.com/kubeedge/kubeedge/cloud/cmd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile Delve</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk add --no-cache git</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> go get github.com/derekparker/delve/cmd/dlv</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> alpine:<span class="number">3.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For debug</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">2345</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> GOARCHAIUS_CONFIG_PATH /etc/kubeedge/cloud</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow delve to run on Alpine based containers.</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk add --no-cache libc6-compat</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> [<span class="string">"/etc/kubeedge/certs"</span>, <span class="string">"/etc/kubeedge/cloud/conf"</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /usr/<span class="built_in">local</span>/bin/edgecontroller /usr/<span class="built_in">local</span>/bin/edgecontroller</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /go/bin/dlv /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/dlv"</span>, <span class="string">"--listen=:2345"</span>, <span class="string">"--headless=true"</span>, <span class="string">"--api-version=2"</span>, <span class="string">"exec"</span>, <span class="string">"/usr/local/bin/edgecontroller"</span>]</span></span><br></pre></td></tr></table></figure><p>Then rebuild the cloudimage from the source project root and save that new image to your docker hub. If you may prefer to skip this step, feel free to grab mine from here <code>r5by/kubeedge_edgecontroller_debug:v1.0.0</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd to your project root and rebuild the cloud image</span></span><br><span class="line">build cloudimage</span><br><span class="line"></span><br><span class="line"><span class="comment"># login to your docker accnt and save the image</span></span><br><span class="line">docker login -u &lt;user_name&gt;</span><br><span class="line">docker tag kubeedge/edgecontroller:v1.0.0 &lt;user_name&gt;/kubeedge_edgecontroller_debug:v1.0.0</span><br><span class="line">docker push &lt;user_name&gt;/kubeedge_edgecontroller_debug:v1.0.0</span><br></pre></td></tr></table></figure><h2 id="Step-4-Connect-with-the-debugger"><a href="#Step-4-Connect-with-the-debugger" class="headerlink" title="Step 4. Connect with the debugger"></a>Step 4. Connect with the debugger</h2><p>The final step is to connect our IDE debugging tool with the container after deployment. First <code>cd build/cloud</code> then modify your <code>07-deployment.yaml</code> as following:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">    kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">      kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line">        <span class="string">container.apparmor.security.beta.kubernetes.io/edgecontroller:</span> <span class="string">unconfined</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">        kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">kubeconfig</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">alpine:3.9</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">kubeconfig</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/etc/kubeedge/cloud</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">-c</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          apk --update add --no-cache coreutils &amp;&amp; cat | tee /etc/kubeedge/cloud/kubeconfig.yaml &lt;&lt;EOF</span></span><br><span class="line"><span class="string"></span><span class="attr">          apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">          kind:</span> <span class="string">Config</span></span><br><span class="line"><span class="attr">          clusters:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">                      cluster:</span></span><br><span class="line"><span class="attr">              certificate-authority-data:</span> <span class="string">$(cat</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span> <span class="string">| base64 -w 0)</span></span><br><span class="line"><span class="string"></span><span class="attr">          users:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">            user:</span></span><br><span class="line"><span class="attr">              token:</span> <span class="string">$(cat</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/token)</span></span><br><span class="line"><span class="attr">          contexts:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">            context:</span></span><br><span class="line"><span class="attr">              cluster:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">              user:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">          current-context:</span> <span class="string">kubeedge</span></span><br><span class="line">          <span class="string">EOF</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">r5by/kubeedge_edgecontroller:v1.0.0</span></span><br><span class="line"><span class="attr">        securityContext:</span></span><br><span class="line"><span class="attr">          capabilities:</span></span><br><span class="line"><span class="attr">            add:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">SYS_PTRACE</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">cloudhub</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">200</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">512</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">conf</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/etc/kubeedge/cloud/conf</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">certs</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/etc/kubeedge/certs</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">kubeconfig</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/etc/kubeedge/cloud</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">conf</span></span><br><span class="line"><span class="attr">        configMap:</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">certs</span></span><br><span class="line"><span class="attr">        secret:</span></span><br><span class="line"><span class="attr">          secretName:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">kubeconfig</span></span><br><span class="line"><span class="attr">        emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p>Then create (if not existing) a <code>08-service.yaml</code> file and copy-paste the following content to it:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">    kubeedge:</span> <span class="string">edgecontroller</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">cloudhub</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">debug</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">2345</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">32345</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubeedge</span></span><br><span class="line"><span class="attr">    kubeedge:</span> <span class="string">edgecontroller</span></span><br></pre></td></tr></table></figure><p>Edit a shell script as following within <code>build/cloud</code> and execute it:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> resource <span class="keyword">in</span> $(ls *.yaml)</span><br><span class="line">    <span class="keyword">do</span> k3s kubectl create -f <span class="variable">$resource</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Note</strong>: To use this script, you need to have a k3s master configured properly and secrets files generated. If you are not sure how to do so, please refer to my next post on how to deploy the Kubeedge cloud core then come back to follow the rest.</p></blockquote><p>Verify the edgecontroller service’s up with the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">k3s kubectl get pod -n kubeedge</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># NAME                              READY   STATUS    RESTARTS   AGE</span></span><br><span class="line"><span class="comment"># edgecontroller-85cdc9cf8f-2p8mj   1/1     Running   0          11s</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check its log</span></span><br><span class="line">k3s kubectl logs edgecontroller-85cdc9cf8f-2p8mj -n kubeedge -c edgecontroller</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># API server listening at: [::]:2345</span></span><br></pre></td></tr></table></figure><p>Now configure your IDE as following:</p><p><img src="https://drive.google.com/uc?export=view&id=1pScAXizLDmS6-K8EgqNnsrPt2kpQlo_t" alt="image"></p><p>Click the debug button and start code walking:</p><p><img src="https://drive.google.com/uc?export=view&id=1f81SmNSQOf96pMF4DYajHNvp0TN_Plea" alt="image"></p><h2 id="Step-5-Trouble-Shooting"><a href="#Step-5-Trouble-Shooting" class="headerlink" title="Step 5. Trouble Shooting"></a>Step 5. Trouble Shooting</h2><p>It is likely that you may confront several errors until successfully connect your debugger to the container. If that happens, check the following tips to see whether it may help you out:</p><blockquote><p>1) The pod was failed due to some “CrushLoopBackOff” status or stuck at “Initializing” </p></blockquote><p>Check your CoreDNS service first, it’s likely that it’s not working so the pulling from public repository failed or stuck. Using the following commands to check:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'kc' alias 'k3s kubectl'</span></span><br><span class="line"><span class="comment"># Verify your coredns is up&amp;running</span></span><br><span class="line">kc get pod --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test your dns resolver</span></span><br><span class="line">kc run -i --tty busybox --image=busybox --restart=Never -- sh</span><br><span class="line"><span class="comment"># Within the busybox</span></span><br><span class="line">vi /etc/resolv.conf <span class="comment">#nameserver, etc.</span></span><br><span class="line">nslookup www.google.com <span class="comment">#nslookup test</span></span><br><span class="line"><span class="comment"># You should be able to see the server and address listed, if not, use the following commands to:</span></span><br><span class="line"><span class="comment"># 1) Verify each Chain of the iptable is set at "ACCEPT" flag</span></span><br><span class="line">iptables -L | grep INPUT  <span class="comment"># check respectively INPUT/OUTPUT/FORWARD in your iptable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) If the above check passed, flush the iptables with following commands to see if error messages are gone</span></span><br><span class="line">iptables --flush</span><br><span class="line">iptables -tnat --flush</span><br><span class="line"></span><br><span class="line"><span class="comment"># If solved, delete the busybox</span></span><br><span class="line">kc delete pod busybox</span><br></pre></td></tr></table></figure><blockquote><p>2) If you see more errors, please refer to my next post on setting up kubeedge cloud part to see if they may be gone. Some errors may be caused by incorrect configurations/running environment.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9naXRodWIuY29tL2t1YmVlZGdlL2t1YmVlZGdl&quot; title=&quot;https://github.com/kubeedge/kubeedge&quot;&gt;Kubeedge&lt;i 
      
    
    </summary>
    
    
      <category term="edge computing, cloud computing" scheme="https://ruby-.github.io/categories/edge-computing-cloud-computing/"/>
    
    
      <category term="kubeedge, k3s" scheme="https://ruby-.github.io/tags/kubeedge-k3s/"/>
    
  </entry>
  
  <entry>
    <title>k3s Dev (2) Vagrant + VirtualBox Dev.</title>
    <link href="https://ruby-.github.io/2019/10/31/k3s-dev-2/"/>
    <id>https://ruby-.github.io/2019/10/31/k3s-dev-2/</id>
    <published>2019-10-30T19:19:14.000Z</published>
    <updated>2019-10-30T22:34:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post aims to help you gain a better development experience with rancher/k3s project. As an old Chinese saying implies: “A logger should always sharpen his axe before going to do his job” (工欲善其事，必先利其器). So in what follows, you may gain experiences on:</p><ul><li>Adopt k3s’s Vagrantfile to launch our virtual dev environment</li><li>Attach the dlv process and start debugging</li></ul><blockquote><p>Special thanks to Eric@RancherLabs who helps me to get to know about his graceful solution on this. Please follow this portal to admire his other works of art: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VyaWt3aWxzb24/dGFiPXJlcG9zaXRvcmllcw==" title="https://github.com/erikwilson?tab=repositories">大神の傳送門<i class="fa fa-external-link"></i></span>.</p></blockquote><h2 id="1-Use-Vagrant"><a href="#1-Use-Vagrant" class="headerlink" title="1. Use Vagrant"></a>1. Use Vagrant</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cudmFncmFudHVwLmNvbS8=" title="https://www.vagrantup.com/">Vagrant<i class="fa fa-external-link"></i></span> is not something new but I thought before that it was merely a VM management tool thus overlooked at this technique due to my ignorance. Until now I realize it’s extremely useful for working at an open-source project, or other similar projects that requires people from different locations to work collaboratively. This is because it offers the developer a method to “ship” his/her working environment directly to all other co-workers, in other words, this concept of “virtual dev env” enables consistency among all developers working at the same project.</p><blockquote><p>The idea of Vagrant is quite similar to Docker, for both provides certain degree of “consistency” and “isolation” in my opinion. However, they are built on top of different tech stacks (virtualization vs. container) and each has its own use-case scenario. Eric also pointed out that “Dapper is nice for building &amp; ci but kind of a pain for development”.</p></blockquote><p>To install Vagrant on MacOS, I recommend homebrew, simply issue the following commends in your terminal:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brew install vagrant</span><br><span class="line"><span class="comment"># or `brew cask install vagrant`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check available commands</span></span><br><span class="line">vagrant -h</span><br></pre></td></tr></table></figure><p>The following list shows some commonly used commends in my case:</p><ul><li><code>vagrant up</code>: spin up the boxes;</li><li><code>vagrant status</code>: check the status of the vagrant machine;</li><li><code>vagrant ssh</code>: ssh into the running machine;</li><li><code>vagrant halt</code>: shutdown the running boxes;</li><li><code>vagrant destroy</code>: delete all associated files to your configured vagrant machines.</li></ul><blockquote><ul><li>(1) Vagrant supports other VM softwares all cross platforms, for me I continue to use VirtualBox here since it’s free and I have had already certain experience with it.</li><li>(2) Issue the <code>destroy</code> command will remove the files for your virtual machine saved in your VirtualBox’s settings, i.e. “/yourpathto/VirtualBox VMS/xxx”.</li></ul></blockquote><p>After learning about the basics, let’s take a look at the Vagrantfile in the k3s project root, to see what does:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">BOX = <span class="string">"generic/alpine310"</span></span><br><span class="line">HOME = File.dirname(__FILE_<span class="number">_</span>)</span><br><span class="line">PROJECT = File.basename(HOME)</span><br><span class="line">MOUNT_TYPE = ENV[<span class="string">'MOUNT_TYPE'</span>] <span class="params">||</span> <span class="string">"nfs"</span></span><br><span class="line">NUM_NODES = (ENV[<span class="string">'NUM_NODES'</span>] <span class="params">||</span> <span class="number">0</span>).to_i</span><br><span class="line">NODE_CPUS = (ENV[<span class="string">'NODE_CPUS'</span>] <span class="params">||</span> <span class="number">4</span>).to_i</span><br><span class="line">NODE_MEMORY = (ENV[<span class="string">'NODE_MEMORY'</span>] <span class="params">||</span> <span class="number">8192</span>).to_i</span><br><span class="line">NETWORK_PREFIX = ENV[<span class="string">'NETWORK_PREFIX'</span>] <span class="params">||</span> <span class="string">"10.135.135"</span></span><br><span class="line">VAGRANT_PROVISION = ENV[<span class="string">'VAGRANT_PROVISION'</span>] <span class="params">||</span> <span class="string">"./scripts/vagrant-provision"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Rules for /etc/sudoers to avoid password entry configuring NFS:</span></span><br><span class="line"><span class="comment"># %adminALL = (root) NOPASSWD: /usr/bin/sed -E -e * -ibak /etc/exports</span></span><br><span class="line"><span class="comment"># %adminALL = (root) NOPASSWD: /usr/bin/tee -a /etc/exports</span></span><br><span class="line"><span class="comment"># %adminALL = (root) NOPASSWD: /sbin/nfsd restart</span></span><br><span class="line"><span class="comment"># --- May need to add terminal to System Preferences -&gt; Security &amp; Privacy -&gt; Privacy -&gt; Full Disk Access</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Check for missing plugins</span></span><br><span class="line">required_plugins = <span class="string">%w( vagrant-alpine vagrant-timezone )</span></span><br><span class="line">plugin_installed = <span class="literal">false</span></span><br><span class="line">required_plugins.each <span class="keyword">do</span> <span class="params">|plugin|</span></span><br><span class="line">  <span class="keyword">unless</span> Vagrant.has_plugin?(plugin)</span><br><span class="line">    system <span class="string">"vagrant plugin install <span class="subst">#&#123;plugin&#125;</span>"</span></span><br><span class="line">    plugin_installed = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment"># --- If new plugins installed, restart Vagrant process</span></span><br><span class="line"><span class="keyword">if</span> plugin_installed === <span class="literal">true</span></span><br><span class="line">  exec <span class="string">"vagrant <span class="subst">#&#123;ARGV.join<span class="string">' '</span>&#125;</span>"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">provision = &lt;&lt;SCRIPT</span><br><span class="line"><span class="comment"># --- Use system gopath if available</span></span><br><span class="line">export GOPATH=<span class="comment">#&#123;ENV['GOPATH']&#125;</span></span><br><span class="line"><span class="comment"># --- Default to root user for vagrant ssh</span></span><br><span class="line">cat &lt;&lt;\\EOF &gt;<span class="regexp">/etc/profile</span>.d/root.sh</span><br><span class="line">[ $EUID -ne <span class="number">0</span> ] &amp;&amp; exec sudo -i</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># --- Set home to current directory</span></span><br><span class="line">cat &lt;&lt;\\EOF &gt;<span class="regexp">/etc/profile</span>.d/home.sh</span><br><span class="line">export HOME=<span class="string">"<span class="subst">#&#123;HOME&#125;</span>"</span> &amp;&amp; cd</span><br><span class="line">EOF</span><br><span class="line">. /etc/profile.d/home.sh</span><br><span class="line"><span class="comment"># --- Run vagrant provision script if available</span></span><br><span class="line"><span class="keyword">if</span> [ ! -x <span class="comment">#&#123;VAGRANT_PROVISION&#125; ]; then</span></span><br><span class="line">  echo <span class="string">'WARNING: Unable to execute provision script "<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>"'</span></span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line">echo <span class="string">"running '<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>'..."</span> &amp;&amp; \</span><br><span class="line">  <span class="comment">#&#123;VAGRANT_PROVISION&#125; &amp;&amp; \</span></span><br><span class="line">  echo <span class="string">"finished '<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>'!"</span></span><br><span class="line">SCRIPT</span><br><span class="line"></span><br><span class="line">Vagrant.configure(<span class="string">"2"</span>) <span class="keyword">do</span> <span class="params">|config|</span></span><br><span class="line">  config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> <span class="params">|v|</span></span><br><span class="line">    v.cpus = NODE_CPUS</span><br><span class="line">    v.memory = NODE_MEMORY</span><br><span class="line">    v.customize [<span class="string">"modifyvm"</span>, <span class="symbol">:id</span>, <span class="string">"--audio"</span>, <span class="string">"none"</span>]</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  config.vm.box = BOX</span><br><span class="line">  config.vm.hostname = PROJECT</span><br><span class="line">  config.vm.synced_folder <span class="string">"."</span>, HOME, <span class="symbol">type:</span> MOUNT_TYPE</span><br><span class="line">  config.vm.provision <span class="string">"shell"</span>, <span class="symbol">inline:</span> provision</span><br><span class="line">  config.timezone.value = <span class="symbol">:host</span></span><br><span class="line"></span><br><span class="line">  config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"<span class="subst">#&#123;NETWORK_PREFIX&#125;</span>.100"</span> <span class="keyword">if</span> NUM_NODES==<span class="number">0</span></span><br><span class="line"></span><br><span class="line">  (<span class="number">1</span>..NUM_NODES).each <span class="keyword">do</span> <span class="params">|i|</span></span><br><span class="line">    config.vm.define <span class="string">".<span class="subst">#&#123;i&#125;</span>"</span> <span class="keyword">do</span> <span class="params">|node|</span></span><br><span class="line">      node.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"<span class="subst">#&#123;NETWORK_PREFIX&#125;</span>.<span class="subst">#&#123;<span class="number">100</span>+i&#125;</span>"</span></span><br><span class="line">      node.vm.hostname = <span class="string">"<span class="subst">#&#123;PROJECT&#125;</span>-<span class="subst">#&#123;i&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>As we can see, mainly what it does is to pull the base box image and then prepare it by evoking the provision script and setting up the network. In particular our case, we need to:</p><ul><li><p>(1) Set up the environment variables to bring up vagrant boxes as we want. For example:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># how many node do we want</span></span><br><span class="line"><span class="built_in">export</span> NUM_NODES=2</span><br><span class="line"></span><br><span class="line"><span class="comment"># how many cpus we have for each node</span></span><br><span class="line"><span class="built_in">export</span> NODE_CPUS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Also can be customized include the network, memory, etc.</span></span><br><span class="line"><span class="comment"># Finally launch the boxes with the above configurations</span></span><br><span class="line">vagrant up</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the running machines</span></span><br><span class="line">vagrant status</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connect to one of the above machines</span></span><br><span class="line">vagrant ssh .1</span><br></pre></td></tr></table></figure></li><li><p>(2) The actually box image that has pulled down and configured is saved at: <code>$HOME/.vagrant.d/boxes</code>.</p></li><li><p>(3) To see what vagrant actually does in the <code>ssh</code> procedure, open another terminal then issue <code>ps aux | grep ssh</code> to verify it, then connect to that node again in another terminal by using that command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh vagrant@127.0.0.1 -p 2222 -o LogLevel=FATAL -o Compression=yes -o DSAAuthentication=yes -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /yourpathto/k3s/.vagrant/machines/.1/virtualbox/private_key</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-Debugging"><a href="#2-Debugging" class="headerlink" title="2. Debugging"></a>2. Debugging</h2><p>After admiring vagrant, destroy these existing boxes and re-prepare them for enabling delve debugger by adding just one line to the <code>scripts/vagrant-provision</code> file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -ve</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> $(dirname <span class="variable">$0</span>)/..</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line">mkdir -p <span class="variable">$&#123;GOPATH&#125;</span>/bin</span><br><span class="line">mkdir -p /go</span><br><span class="line">ln -s <span class="variable">$GOPATH</span>/bin /go/bin</span><br><span class="line">sed <span class="string">':a;N;$!ba;s/\\\n/ /g'</span> &lt;Dockerfile.dapper | grep <span class="string">'^RUN '</span> | sed -e <span class="string">'s/^RUN //'</span> &gt;/tmp/docker-run</span><br><span class="line"><span class="built_in">export</span> BINDIR=/go/bin</span><br><span class="line"><span class="built_in">export</span> GOPATH=/go</span><br><span class="line"><span class="built_in">export</span> HOME=/tmp &amp;&amp; <span class="built_in">cd</span></span><br><span class="line">. /tmp/docker-run</span><br><span class="line"><span class="built_in">cd</span> /go</span><br><span class="line">go get github.com/rancher/trash</span><br><span class="line"><span class="comment"># --- Add one line here to enable delve           &lt;==</span></span><br><span class="line">go get -u github.com/go-delve/delve/cmd/dlv</span><br><span class="line">rm -rf /go</span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Now ssh into the virtual machine and start debugging as we already learned in the previous post:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After inspecting the vagrant, close and destroy it</span></span><br><span class="line"><span class="comment"># or stop by `vagrant halt`</span></span><br><span class="line">vagrant destroy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding dlv to the provision script then reload the vagrant</span></span><br><span class="line">vagrant reload</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build from the source</span></span><br><span class="line">./scripts/download &amp;&amp; ./scripts/build &amp;&amp; ./scripts/package-cli</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch dlv</span></span><br><span class="line">dlv --listen=:2345 --headless=<span class="literal">true</span> --api-version=2 --accept-multiclient <span class="built_in">exec</span> dist/artifacts/k3s -- --debug server</span><br></pre></td></tr></table></figure><blockquote><p><strong>Note</strong>: Use <code>netstat</code> you will notice some nfs related daemons, these rpc procedures are critical to maintain the consistency between the host’s source files and those to be built in virtual dev boxes. Also, to kill the dlv debugger simply kill the process from a different terminal. The most enhanced experience (at least for me) is that any modification on the source code in my host will be directly synchronized to the virtual box side via the mount, bravo!</p></blockquote><h2 id="3-Summary"><a href="#3-Summary" class="headerlink" title="3. Summary"></a>3. Summary</h2><p>In this post we have used Vagrant and Virtualbox to set up our dev environment. In my following posts, we’ll continue to dig deeper into k3s’ source code and learn more about it soon.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post aims to help you gain a better development experience with rancher/k3s project. As an old Chinese saying implies: “A logger sho
      
    
    </summary>
    
    
      <category term="edge computing" scheme="https://ruby-.github.io/categories/edge-computing/"/>
    
    
      <category term="k3s, k8s" scheme="https://ruby-.github.io/tags/k3s-k8s/"/>
    
  </entry>
  
  <entry>
    <title>k3s Dev (1) Environment Setup</title>
    <link href="https://ruby-.github.io/2019/10/11/k3s-dev-1/"/>
    <id>https://ruby-.github.io/2019/10/11/k3s-dev-1/</id>
    <published>2019-10-10T20:07:36.000Z</published>
    <updated>2019-10-30T22:28:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post shows in steps how to set up a dev environment for <strong>k3s</strong> project.</p><h2 id="Step-1-Preparation"><a href="#Step-1-Preparation" class="headerlink" title="Step 1. Preparation"></a>Step 1. Preparation</h2><p>The following list shows my local development environment setup:</p><p>For code review &amp; build purpose:</p><ul><li>OS: Mac OS </li><li>Docker: 18.06.1-ce</li><li>JetBrains GoLand IDE</li><li>golang: 1.12.7</li></ul><p>For testing deploy purpose:</p><ul><li>VirtualBox</li><li>Ubuntu 18.04 Server</li></ul><blockquote><p>Alternatively, one could use EC2 instances as testing deploy environment, in which case may directly to go step 2.</p></blockquote><h3 id="1-1-VirtualBox-Setup"><a href="#1-1-VirtualBox-Setup" class="headerlink" title="1.1 VirtualBox Setup"></a>1.1 VirtualBox Setup</h3><p>Download Ubuntu 18.04 iso from <span class="exturl" data-url="aHR0cDovL3JlbGVhc2VzLnVidW50dS5jb20vMTguMDQv" title="http://releases.ubuntu.com/18.04/">official source<i class="fa fa-external-link"></i></span> and install it on VirtualBox. Configure the network interface as following:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check all available network adapters</span></span><br><span class="line">ifconfig -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then Use netplan to config all interfaces</span></span><br><span class="line">sudo vi /etc/netplan/50-cloud-init.yaml</span><br></pre></td></tr></table></figure><p>Change the content accordingly (192.168.56.1 is my gateway setting, change it according to your own VirtualBox network setting):</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line"><span class="attr">    ethernets:</span></span><br><span class="line"><span class="attr">        enp0s3:</span></span><br><span class="line"><span class="attr">            addresses:</span> <span class="string">[]</span></span><br><span class="line"><span class="attr">            dhcp4:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            dhcp6:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">            optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            nameservers:</span></span><br><span class="line"><span class="attr">               addresses:</span> <span class="string">[8.8.8.8]</span></span><br><span class="line"><span class="attr">        enp0s8:</span></span><br><span class="line"><span class="attr">             addresses:</span> <span class="string">[192.168.56.10/24]</span></span><br><span class="line"><span class="attr">             dhcp4:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">             dhcp6:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">             optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">             routes:</span></span><br><span class="line"><span class="attr">                - to:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span><span class="string">/24</span></span><br><span class="line"><span class="attr">                  via:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span></span><br><span class="line"><span class="attr">    version:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><p>Save and apply the changes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo netplan apply</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test connectivity</span></span><br><span class="line">ping 192.168.56.101 <span class="comment"># another virtual machine</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test installing k3s</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> sh -</span><br></pre></td></tr></table></figure><h3 id="1-2-Golang-Environment"><a href="#1-2-Golang-Environment" class="headerlink" title="1.2 Golang Environment"></a>1.2 Golang Environment</h3><p>For Mac OS users, I recommend use <code>homebrew</code> to manage go environment. However, go has its own version manager called GVM, refer <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21vb3Z3ZWIvZ3Zt" title="https://github.com/moovweb/gvm">here<i class="fa fa-external-link"></i></span> for detailed information. To use earlier version of go, add following to your <code>.bashrc</code> file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># homebrew go version config </span></span><br><span class="line"><span class="function"><span class="title">goconfig</span></span>() &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></span><br><span class="line">        <span class="string">"1.12"</span>)</span><br><span class="line">            brew switch go 1.12.7</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.12.7/libexec</span><br><span class="line">            ;;</span><br><span class="line">        <span class="string">"1.13"</span>)</span><br><span class="line">            brew switch go 1.13</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.13/libexec</span><br><span class="line">            ;;</span><br><span class="line">        *)</span><br><span class="line">            brew switch go 1.12.7</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.12.7/libexec</span><br><span class="line">            ;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Installed Go path</span></span><br><span class="line">    <span class="built_in">export</span> GOPATH=<span class="variable">$HOME</span>/go</span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:GOROOT/bin</span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:$(go env GOPATH)/bin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>To use simply pass the version selected. For example, to change to go version 12:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Switch to go version 12</span></span><br><span class="line">goconfig 1.12</span><br></pre></td></tr></table></figure><h3 id="1-3-Source-Code-Download"><a href="#1-3-Source-Code-Download" class="headerlink" title="1.3 Source Code Download"></a>1.3 Source Code Download</h3><p>With go environment set up, now we go to its workspace to pull the source from github (I have already forked k3s project to my own github account, if that’s not the case for you, simply pull the source from <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNz" title="https://github.com/rancher/k3s">rancher<i class="fa fa-external-link"></i></span>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /Users/yourname/go/src/github.com/yourgithubacct</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download my forked project to local</span></span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/ruby-/k3s.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add rancher remote source</span></span><br><span class="line">git remote add rancher https://github.com/rancher/k3s.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># now there should be two remote sources: origin (my own) and rancher's</span></span><br><span class="line">git remote</span><br><span class="line"></span><br><span class="line"><span class="comment"># fetch from rancher's master branch and merge to local (shouldn't be any change yet)</span></span><br><span class="line">git fetch rancher</span><br><span class="line">git merge rancher/master</span><br></pre></td></tr></table></figure><h2 id="Step-2-Build-amp-Test-Launch"><a href="#Step-2-Build-amp-Test-Launch" class="headerlink" title="Step 2. Build &amp; Test Launch"></a>Step 2. Build &amp; Test Launch</h2><p>Open the k3s folder in GoLand IDE, on Mac OS, press <code>⌘ + ⇧ + f</code> to search in path for <em>“k3s is up and running”</em>, open the <em>pkg/cli/server/server.go</em> file and add following statement:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">run</span><span class="params">(app *cli.Context, cfg *cmds.Server)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    ctx := signals.SetupSignalHandler(context.Background())</span><br><span class="line">    certs, err := server.StartServer(ctx, &amp;serverConfig)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    logrus.Info(<span class="string">"k3s is up and running"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Add a new logging info here</span></span><br><span class="line">    logrus.Info(<span class="string">"==================&gt; Fog is just clouds that have fell down! &lt;=================="</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> notifySocket != <span class="string">""</span> &#123;</span><br><span class="line">    os.Setenv(<span class="string">"NOTIFY_SOCKET"</span>, notifySocket)</span><br><span class="line">        systemd.SdNotify(<span class="literal">true</span>, <span class="string">"READY=1\n"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now open the terminal within GoLand IDE, then issue <code>make</code> command and wait for it’s pulling required docker images and build the project for us. After the building procedure, there should be <strong>hyperkube</strong>, <strong>k3s</strong> produced in the <em>dist/artifacts/</em> folder. Simply copy <strong>k3s</strong> to your deploy environment for testing purpose (for me only need to copy to virtualbox ubuntu server that has been setup):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp dist/artifacts/k3s main@192.168.56.10:~/k3s/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start k3s server and verify the added logged info</span></span><br><span class="line">k3s server &gt; server.log 2&gt;&amp;1 &amp;</span><br><span class="line">vi server.log</span><br></pre></td></tr></table></figure><blockquote><p>Note that:</p><ul><li>(1) Remember to <code>chmod 777 k3s_install_path</code> the install path for <code>scp</code> to be able to upload k3s executable; </li><li>(2) k3s doesn’t support cross compile at current release, refer to <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNzL2lzc3Vlcy81MzA=" title="https://github.com/rancher/k3s/issues/530">issue#530<i class="fa fa-external-link"></i></span> for detailed discussion regarding this;</li><li>(3) There are two options for developers to build k3s from source. We are adapting docker method with <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvZGFwcGVy" title="https://github.com/rancher/dapper">rancher/dapper<i class="fa fa-external-link"></i></span> wrapper in this post. However for those who are working on Linux, may alternatively download the dependencies and directly build the source with <code>go build</code> commend described <span class="exturl" data-url="aHR0cHM6Ly9yYW5jaGVyLmNvbS9kb2NzL2szcy9sYXRlc3QvZW4vYnVpbGRpbmcv" title="https://rancher.com/docs/k3s/latest/en/building/">here<i class="fa fa-external-link"></i></span>; unfortunately this compile option is not available for Mac OS users at current release, refer to <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNzL2lzc3Vlcy8yNzM=" title="https://github.com/rancher/k3s/issues/273">issue#273<i class="fa fa-external-link"></i></span> for detailed discussion regarding this.</li><li>(4) For Mac OS users, docker tends to accumulate its data file (store at <code>/Users/yourname/Library/Containers/com.docker.docker/Data/vms/0/Docker.qcow2</code>) which contains its self os image and downloaded containers. Build <strong>k3s</strong> project may consume ~10GB space on disk and it keeps growing. The way to get around this is to use docker built-in “reset” tab (Also note that <code>docker system prune</code> won’t save you from this pitfall).</li></ul></blockquote><h2 id="Step-3-Breakpoint-amp-debugging"><a href="#Step-3-Breakpoint-amp-debugging" class="headerlink" title="Step 3. Breakpoint &amp; debugging"></a>Step 3. Breakpoint &amp; debugging</h2><p>It is essential for us engineers to be able to set up breakpoint and debug the code in software development. In this section we’ll try to connect to remotely deployed <strong>k3s</strong> executable and use <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvLWRlbHZlL2RlbHZl" title="https://github.com/go-delve/delve">delve<i class="fa fa-external-link"></i></span> tool to navigate through the debugging process. </p><h3 id="3-1-Install-go-and-dlv-in-deploy-env"><a href="#3-1-Install-go-and-dlv-in-deploy-env" class="headerlink" title="3.1 Install go and dlv in deploy env"></a>3.1 Install go and dlv in deploy env</h3><p>To install and set up go environment and dlv debugger on Ubuntu 18.04 server VM we created earlier, simply follow these two guides:</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGlnaXRhbG9jZWFuLmNvbS9jb21tdW5pdHkvdHV0b3JpYWxzL2hvdy10by1pbnN0YWxsLWdvLW9uLXVidW50dS0xOC0wNA==" title="https://www.digitalocean.com/community/tutorials/how-to-install-go-on-ubuntu-18-04">How To Install Go on Ubuntu 18.04<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvLWRlbHZlL2RlbHZlL2Jsb2IvbWFzdGVyL0RvY3VtZW50YXRpb24vaW5zdGFsbGF0aW9uL2xpbnV4L2luc3RhbGwubWQ=" title="https://github.com/go-delve/delve/blob/master/Documentation/installation/linux/install.md">dlv - Installation on Linux<i class="fa fa-external-link"></i></span></li></ul><p>To get familiar with <code>dlv</code> basic workflows and combine it with JetBrain GoLand IDE, I recommend to follow these practices:</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuamFtZXNzdHVydGV2YW50LmNvbS9wb3N0cy9Vc2luZy10aGUtR28tRGVsdmUtRGVidWdnZXItZnJvbS10aGUtY29tbWFuZC1saW5lLw==" title="https://www.jamessturtevant.com/posts/Using-the-Go-Delve-Debugger-from-the-command-line/">Using the Go Delve Debugger from the command line<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmpldGJyYWlucy5jb20vZ28vMjAxOS8wMi8wNi9kZWJ1Z2dpbmctd2l0aC1nb2xhbmQtZ2V0dGluZy1zdGFydGVkLw==" title="https://blog.jetbrains.com/go/2019/02/06/debugging-with-goland-getting-started/">Debugging with GoLand – Getting Started<i class="fa fa-external-link"></i></span></li></ul><p>Now let us go back to our <strong>k3s</strong> project, find and modify the following lines in <code>scripts/build.sh</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">VERSIONFLAGS=<span class="string">"</span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/pkg/version.Version=<span class="variable">$VERSION</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/pkg/version.GitCommit=<span class="variable">$&#123;COMMIT:0:8&#125;</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CONTAINERD</span>/version.Version=<span class="variable">$VERSION_CONTAINERD</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CONTAINERD</span>/version.Package=<span class="variable">$PKG_RANCHER_CONTAINERD</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CRICTL</span>/pkg/version.Version=<span class="variable">$VERSION_CRICTL</span>"</span></span><br><span class="line"><span class="comment"># Comment out LDFLAGS (link) for debug: "-w"= wipe out debug info ;; "-s"= remove symbol table</span></span><br><span class="line"><span class="comment"># LDFLAGS="</span></span><br><span class="line"><span class="comment">#    -w -s"</span></span><br><span class="line">LDFLAGS=<span class="string">""</span></span><br><span class="line">STATIC=<span class="string">"</span></span><br><span class="line"><span class="string">    -extldflags '-static'</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Go to <code>scripts/package-cli.sh</code> file and change the following:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">go generate</span><br><span class="line">LDFLAGS=<span class="string">"</span></span><br><span class="line"><span class="string">    -X github.com/rancher/k3s/pkg/version.Version=<span class="variable">$VERSION</span></span></span><br><span class="line"><span class="string">    -X github.com/rancher/k3s/pkg/version.GitCommit=<span class="variable">$&#123;COMMIT:0:8&#125;</span></span></span><br><span class="line"><span class="string">    -w -s</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line">STATIC=<span class="string">"-extldflags '-static'"</span></span><br><span class="line"><span class="comment"># CGO_ENABLED=0 go build -ldflags "$LDFLAGS $STATIC" -o $&#123;CMD_NAME&#125; ./cmd/k3s/main.go</span></span><br><span class="line"><span class="comment"># Add gcflags to enable dlv</span></span><br><span class="line">CGO_ENABLED=0 go build -gcflags <span class="string">"all=-N -l"</span> -ldflags <span class="string">"<span class="variable">$STATIC</span>"</span> -o <span class="variable">$&#123;CMD_NAME&#125;</span> ./cmd/k3s/main.go</span><br></pre></td></tr></table></figure><p>Commit the file changes to our local git repo and rebuild the project, then upload the new <strong>k3s</strong> executable to our local VM:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Commit local changes</span></span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">"enable dlv"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rebuild project</span></span><br><span class="line">make</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload to deploy</span></span><br><span class="line">scp dist/artifacts/k3s main@192.168.56.10:~/k3s/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch with dlv: note `dlv exec xxx -- paras` means passing the parameters after "--" to our executable instead of dlv</span></span><br><span class="line">dlv --listen=:2345 --headless=<span class="literal">true</span> --api-version=2 --accept-multiclient <span class="built_in">exec</span> /home/main/k3s/k3s -- --debug server</span><br></pre></td></tr></table></figure><p>Setup a new go remote debug run/debug configuration in GoLand IDE which looks like this:<br><img src="https://drive.google.com/uc?export=view&id=1VTjvnD_Fi3o1kGt3hc_o-IyeubWb5bTX" alt="image"></p><p>Toggle some breakpoint at main entry then click <strong>debug</strong> button to start debugging.<br><img src="https://drive.google.com/uc?export=view&id=1Yf24i0WFEMJR-mnvyOg79X_xcnqx_06t" alt="image"></p><blockquote><p>Some of my questions:</p><ul><li>(1) <del>Use the <code>make</code> method to build the whole project takes long time (3-5 min for me). I wonder if there’s some “quick” and “convenient” way to speedup this procedure, e.g. build only parts that have been changed? Please leave comments below if you know the answer to this.</del><br>(Updated) Please refer my following post for a better way to separate source code edit &amp; build/test environment here at <a href="/2019/10/30/k3s-dev-2/" title="k3s Dev (2) Vagrant + VirtualBox Dev.">k3s Dev (2) Vagrant + VirtualBox Dev.</a></li><li>(2) The latest stable version of GoLand(2019.2) leave the channel open after the debugging procedure has been stopped within IDE. Simple kill the dlv by PID to get around this issue.</li><li>(3) Run <code>k3s</code> command with generate a bunch of code in <code>/var/lib/rancher/k3s/</code>, these run time generated binaries and configurations contain most important components including <code>k3s-server</code> and <code>k3s-agent</code>. Details about these we’ll explore also in next my post.</li></ul></blockquote><h2 id="Summery"><a href="#Summery" class="headerlink" title="Summery"></a>Summery</h2><p>Learning k3s from source is important for us to contribute to its community. In my next post, I’ll try to investigate its scheduling mechanism and explain the related major workflow.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post shows in steps how to set up a dev environment for &lt;strong&gt;k3s&lt;/strong&gt; project.&lt;/p&gt;
&lt;h2 id=&quot;Step-1-Preparation&quot;&gt;&lt;a href=&quot;#Step
      
    
    </summary>
    
    
      <category term="edge computing" scheme="https://ruby-.github.io/categories/edge-computing/"/>
    
    
      <category term="k3s, k8s" scheme="https://ruby-.github.io/tags/k3s-k8s/"/>
    
  </entry>
  
  <entry>
    <title>k3s Basics</title>
    <link href="https://ruby-.github.io/2019/09/30/k3s-basics/"/>
    <id>https://ruby-.github.io/2019/09/30/k3s-basics/</id>
    <published>2019-09-29T22:45:11.000Z</published>
    <updated>2019-10-12T22:09:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post explains my first impression with <strong>k3s</strong>.</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In order to work on k3s project, one needs to learn to use it at first. This post summarize the main steps of deploying <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JhbmNoZXIvazNz" title="https://github.com/rancher/k3s">k3s<i class="fa fa-external-link"></i></span> in our local lab environment and also on AWS EC2.</p><h2 id="Local-k3s-cluster-Setup"><a href="#Local-k3s-cluster-Setup" class="headerlink" title="Local k3s cluster Setup"></a>Local k3s cluster Setup</h2><p>The basic hardware setup for this demo is as the same as the last post on k8s demo (3 VMs), feel free to apply the following methods with the latest released k3s on your own Raspbian nodes, it should work as we also tested it. However, the latest released version (0.9.1 at the time of this post) of k3s has some ca-related issues for an agent to join the master node (if a full ca-enabled Kubernetes cluster has been configured on these nodes), thus an earlier release version (0.2.0) was used for this section.</p><h3 id="Step-1-Stop-the-previously-installed-k8s-related-services-optional"><a href="#Step-1-Stop-the-previously-installed-k8s-related-services-optional" class="headerlink" title="Step 1: Stop the previously installed k8s related services (optional)"></a>Step 1: Stop the previously installed k8s related services (optional)</h3><p>On master node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service kube-calico stop</span><br><span class="line">service kube-scheduler stop</span><br><span class="line">service kube-controller-manager stop</span><br><span class="line">service kube-apiserver stop</span><br><span class="line">service etcd stop &amp;&amp; rm -fr /var/lib/etcd/*</span><br></pre></td></tr></table></figure><p>On worker nodes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service kubelet stop &amp;&amp; rm -fr /var/lib/kubelet/*</span><br><span class="line">service kube-proxy stop &amp;&amp; rm -fr /var/lib/kube-proxy/*</span><br><span class="line">service kube-calico stop</span><br></pre></td></tr></table></figure><blockquote><p>Here I simply turn k8s off to avoid potential conflicts between k8s and k3s deployment. However this doesn’t prevent the ca issue for latest k3s releases (&gt;v0.2.0) to work with previously installed k8s environment.</p></blockquote><h3 id="Step-2-Deploy-k3s-on-each-node"><a href="#Step-2-Deploy-k3s-on-each-node" class="headerlink" title="Step 2: Deploy k3s on each node"></a>Step 2: Deploy k3s on each node</h3><p>To install k3s is quite simple, we will use the following commands install and start k3s on master and two worker machines separately. </p><p>On master node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install k3s with rancher script</span></span><br><span class="line"><span class="comment"># The script download the binaries, pull the images (containerd) and enable/start the k3s-related systemctl services</span></span><br><span class="line">mkdir k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># With INSTALL_K3S_EXEC="--disable-agent" option, one may launch k3s server on the node without an agent (which may cause some issues with current release of k3s)</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> INSTALL_K3S_VERSION=<span class="string">"v0.2.0"</span>  sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the k3s system services are listening on their ports</span></span><br><span class="line">netstat -nltp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the k3s cluster status</span></span><br><span class="line">systemctl status k3s</span><br></pre></td></tr></table></figure><blockquote><p>On master node, one should see services and their port numbers: <em>k3s : 6443/6444, 10251/10252</em></p></blockquote><p>After installation, the k3s binary folder looks like this:</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── crictl -&gt; k3s</span><br><span class="line">├── k3s</span><br><span class="line">├── k3s-killall.sh</span><br><span class="line">└── k3s-uninstall.sh</span><br></pre></td></tr></table></figure><blockquote><p>With out the INSTALL_K3S_BIN_DIR option, k3s will be installed at /usr/local/bin</p></blockquote><p>Now, in order to join new workers to this master node, one needs first grab the token on that (master) node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example output: K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bb</span></span><br><span class="line">cat /var/lib/rancher/k3s/server/node-token</span><br></pre></td></tr></table></figure><p>On worker nodes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># download &amp; active the k3s-agent service</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> INSTALL_K3S_VERSION=<span class="string">"v0.2.0"</span> K3S_TOKEN=<span class="string">"K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bb"</span> K3S_URL=<span class="string">"https://192.168.56.103:6443"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the service status</span></span><br><span class="line">systemctl status k3s-agent</span><br></pre></td></tr></table></figure><p>The installation of k3s agent on worker nodes looks like this:</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── crictl -&gt; k3s</span><br><span class="line">├── k3s</span><br><span class="line">├── k3s-agent-uninstall.sh</span><br><span class="line">└── k3s-killall.sh</span><br></pre></td></tr></table></figure><blockquote><p>On worker node, k3s has services and ports: <em>k3s : 42323, containerd : 10010</em></p></blockquote><p>Now on the master node, one should be able to verify the newly added cluster resources:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Now verify the newly added worker in the cluster</span></span><br><span class="line">k3s kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">## Do some deployment here...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Clean-up</span></span><br><span class="line"><span class="comment"># Kill k3s services after inspection (on each node)</span></span><br><span class="line">k3s-killall.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uninstall k3s on master</span></span><br><span class="line">k3s-uninstall.sh</span><br><span class="line"><span class="comment"># Uninstall k3s on worker nodes</span></span><br><span class="line">k3s-agent-uninstall.sh</span><br></pre></td></tr></table></figure><h2 id="Install-k3s-manually-on-EC2-instances"><a href="#Install-k3s-manually-on-EC2-instances" class="headerlink" title="Install k3s manually on EC2 instances"></a>Install k3s manually on EC2 instances</h2><p>This section explains how to manually download k3s binaries from rancher’s official release and cluster-up. Here we use AWS EC2 service as following configuration:</p><table><thead><tr><th align="center">Instance OS</th><th align="center">Arch</th><th align="center">IP (internal)</th><th align="center">Instance Type</th><th align="center">vCPU</th><th align="center">Memory</th><th align="center">Node Role</th></tr></thead><tbody><tr><td align="center">Ubuntu Server 18.04 LTS (HVM), SSD Volume Type</td><td align="center">amd64(x86_64)</td><td align="center">172.31.46.70</td><td align="center">t2.medium</td><td align="center">2</td><td align="center">4GiB</td><td align="center">master</td></tr><tr><td align="center">Amazon Linux 2 AMI (HVM), SSD Volume Type</td><td align="center">arm64(aarch64)</td><td align="center">172.31.36.129</td><td align="center">a1.medium</td><td align="center">1</td><td align="center">2GiB</td><td align="center">worker</td></tr></tbody></table><blockquote><p>Remember to allow all traffic from anywhere in your security group setting.</p></blockquote><p>First, let us <code>ssh</code> to each running instance and prepare the k3s executable</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change to your own pem key file and instance address:</span></span><br><span class="line">ssh -i ~/.ssh/your-key.pem ubuntu@ec2-x-x-x-x.region.compute.amazonaws.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare download folder</span></span><br><span class="line">mkdir k3s</span><br><span class="line"><span class="built_in">cd</span> k3s</span><br><span class="line"></span><br><span class="line"><span class="comment">## Download the desired release version from: https://github.com/rancher/k3s/releases?after=v0.10.0-alpha1</span></span><br><span class="line"><span class="comment"># On master (x86_64)</span></span><br><span class="line">wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s</span><br><span class="line"><span class="comment"># On worker (arm)</span></span><br><span class="line">wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s-arm64</span><br><span class="line">mv k3s-arm64 k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add exec mode</span></span><br><span class="line">chmod +x k3s</span><br></pre></td></tr></table></figure><p>On master node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start k3s server</span></span><br><span class="line">./k3s server &gt; server.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get token copy-and-paste the output to your worker:</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export node_token=<span class="variable">$(cat /var/lib/rancher/k3s/server/node-token)</span>"</span></span><br></pre></td></tr></table></figure><p>On worker node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># copy-and-paste token from master here</span></span><br><span class="line"><span class="built_in">export</span> node_token=...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start agent, pass server url and token</span></span><br><span class="line">./k3s agent --server https://172.31.46.70:6443 --token <span class="string">"<span class="variable">$node_token</span>"</span> &gt;&amp; k3s-agent.log &amp;</span><br></pre></td></tr></table></figure><p>After a little while, check the cluster info with <code>k3s kubectl</code> command described in last section on the master node.</p><blockquote><p>Simply kill the PID to stop k3s or k3s-agent in the demo to shut down the cluster after inspection.</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Ranger k3s is much smaller and easier to deploy compared to Kubernetes, and it requires less effort and resources to set up. In next post, we will discuss how to set-up a development environment on k3s and dive deeper to learn k3s from its source code.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post explains my first impression with &lt;strong&gt;k3s&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="edge computing" scheme="https://ruby-.github.io/categories/edge-computing/"/>
    
    
      <category term="k3s, k8s" scheme="https://ruby-.github.io/tags/k3s-k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s Basics</title>
    <link href="https://ruby-.github.io/2019/09/18/k8s-basics/"/>
    <id>https://ruby-.github.io/2019/09/18/k8s-basics/</id>
    <published>2019-09-18T01:56:46.000Z</published>
    <updated>2019-10-12T22:10:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post demonstrate some Kubernetes basic commends.</p><h2 id="Server-Overview"><a href="#Server-Overview" class="headerlink" title="Server Overview"></a>Server Overview</h2><p>We have setup 3 virtual machines，each has 1 cpu and 1GB memory. Details：</p><table><thead><tr><th align="center">Server OS</th><th align="center">IP Address</th><th align="center">Node Type</th><th align="center">CPU</th><th align="center">Memory</th><th align="center">Hostname</th></tr></thead><tbody><tr><td align="center">ubuntu16.04</td><td align="center">192.168.56.103</td><td align="center">master</td><td align="center">1</td><td align="center">1G</td><td align="center">server01</td></tr><tr><td align="center">ubuntu16.04</td><td align="center">192.168.56.104</td><td align="center">slave1</td><td align="center">1</td><td align="center">1G</td><td align="center">server02</td></tr><tr><td align="center">ubuntu16.04</td><td align="center">192.168.56.105</td><td align="center">slave2</td><td align="center">1</td><td align="center">1G</td><td align="center">server03</td></tr></tbody></table><blockquote><p>To follow this demo, root privilege is required, ask system admin (Todd) for root access.</p></blockquote><a id="more"></a><h2 id="Startup-all-nodes"><a href="#Startup-all-nodes" class="headerlink" title="Startup all nodes"></a>Startup all nodes</h2><p>Login to the system then start virtualBox and each virtual machine.</p><h3 id="Launch-VirtualBox"><a href="#Launch-VirtualBox" class="headerlink" title="Launch VirtualBox"></a>Launch VirtualBox</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get root privilege</span></span><br><span class="line">$ sudo -s</span><br><span class="line"><span class="comment"># Then start master, slave1 and slave2 virtual machines </span></span><br><span class="line">$ virtualbox</span><br></pre></td></tr></table></figure><h3 id="Login-to-each-node"><a href="#Login-to-each-node" class="headerlink" title="Login to each node"></a>Login to each node</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ Login as: main</span><br><span class="line"><span class="comment"># passwd: 000000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># And get root priviledge on each node</span></span><br><span class="line">$ sudo -s</span><br></pre></td></tr></table></figure><h3 id="Check-system-information-on-each-node"><a href="#Check-system-information-on-each-node" class="headerlink" title="Check system information on each node"></a>Check system information on each node</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check ip address</span></span><br><span class="line">$ ifconfig</span><br><span class="line"><span class="comment"># check running containers</span></span><br><span class="line">$ docker ps</span><br><span class="line"><span class="comment"># check services &amp; ports</span></span><br><span class="line">$ netstat -nltp</span><br></pre></td></tr></table></figure><blockquote><p>On master node, one should see services and their port numbers: <em>kube-apiserver : 6443/8080, etcd : 2379/2380, kube-scheduler : 10251, kube-controller : 10252, calico-felix : 9099</em></p></blockquote><blockquote><p>On worker nodes, one should see services and their port numbers: <em>kubelet : 4194/10248/10250/10255, kube-proxy:10249/10256, calico-felix : 9099</em></p></blockquote><h2 id="Commonly-used-commands"><a href="#Commonly-used-commands" class="headerlink" title="Commonly used commands"></a>Commonly used commands</h2><h3 id="Use-calico-to-check-the-network-status-on-each-node"><a href="#Use-calico-to-check-the-network-status-on-each-node" class="headerlink" title="Use calico to check the network status on each node"></a>Use calico to check the network status on each node</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ calicoctl node status</span><br></pre></td></tr></table></figure><blockquote><p>At each node, it should be able to see the other two nodes’ ip addresses in the cluster.</p></blockquote><h3 id="Use-kubectl-on-master-node-to-verify-the-cluster-resources-deloyment-nodes-pods-services-etc"><a href="#Use-kubectl-on-master-node-to-verify-the-cluster-resources-deloyment-nodes-pods-services-etc" class="headerlink" title="Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)"></a>Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check server/client version</span></span><br><span class="line">$ kubectl version</span><br><span class="line"><span class="comment"># get workers</span></span><br><span class="line">$ kubectl get node</span><br><span class="line"><span class="comment"># get pods</span></span><br><span class="line">$ kubectl get pods</span><br><span class="line"><span class="comment"># get deployment</span></span><br><span class="line">$ kubectl get deploy</span><br><span class="line"><span class="comment"># get services</span></span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure><h3 id="More-kubectl-commands"><a href="#More-kubectl-commands" class="headerlink" title="More kubectl commands"></a>More kubectl commands</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run kubernetes-bootcamp --image=jocatalin/kubernetes-bootcamp:v1 --port=8080</span><br><span class="line"><span class="comment"># check deploy/pods again</span></span><br><span class="line">$ kubectl get deploy</span><br><span class="line">$ kubectl get pods</span><br><span class="line"><span class="comment"># i.e. NAME: kubernetes-bootcamp-6b7849c495-p7dsw</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then check log of the pod</span></span><br><span class="line">$ kubectl logs kubernetes-bootcamp-6b7849c495-p7dsw -f</span><br><span class="line"><span class="comment"># (ctrl-c out the following log)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># describe pod</span></span><br><span class="line">$ kubectl describe pods kubernetes-bootcamp-6b7849c495-p7dsw</span><br><span class="line"><span class="comment"># (Find the Mounts:/var/run/secrets/kubernetes.io/serviceaccount in the description)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enter the running pod and verify the above path</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it kubernetes-bootcamp-6b7849c495-p7dsw bash</span><br><span class="line"><span class="comment"># Find out the certificate files in that path</span></span><br><span class="line">$ ls -l /var/run/secrets/kubernetes.io/serviceaccount </span><br><span class="line"><span class="comment"># Exit the pod</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"><span class="comment"># These ca files are actually associate with the ca account, check that info by:</span></span><br><span class="line">$ kubectl get sa -o yaml</span><br><span class="line"><span class="comment"># can also output other pattern, e.g. json</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check the "secret"</span></span><br><span class="line">$ kubectl get secrets -o yaml</span><br><span class="line"><span class="comment"># (The content of this secret has 3 sections that are mounted as three files in each pod as we see in above)</span></span><br></pre></td></tr></table></figure><blockquote><p>The “secret” are mounted to each created pod as files located in /var/run/secrets/… so that each pod can connect with api-server with https requests.</p></blockquote><h3 id="Use-‘apply’-or-‘create’-with-yaml-files"><a href="#Use-‘apply’-or-‘create’-with-yaml-files" class="headerlink" title="Use ‘apply’ or ‘create’ with yaml files"></a>Use ‘apply’ or ‘create’ with yaml files</h3><p>Kubectl ‘apply’ command is similar to ‘create’ command, but has rich properties.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> services</span><br><span class="line"><span class="comment"># Create a nginx pod and verify</span></span><br><span class="line">$ kubectl apply -f nginx-pod.yaml</span><br><span class="line">$ kubectl describe pod nginx</span><br><span class="line"><span class="comment"># (version here used is 1.7.9)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vi nginx-pod.yaml and change image:nginx:1.7.9 -&gt; image:nginx:1.13, re-apply the yaml file</span></span><br><span class="line">$ kubectl apply -f nginx-pod.yaml</span><br><span class="line"><span class="comment"># (version here is now 1.13)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to change version of running image is to use 'set' command</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image pods nginx nginx=nginx:1.7.9</span><br><span class="line"><span class="comment"># (reset the version to 1.7.9)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 'apply' command can be also used to create other resources</span></span><br><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line">$ kubectl apply -f nginx-service.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the service is runing</span></span><br><span class="line">$ curl 192.168.56.104:20000</span><br><span class="line">$ curl 192.168.56.105:20000</span><br><span class="line"><span class="comment"># (Nginx welcome page should be displayed)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the service</span></span><br><span class="line">$ kubectl get svc</span><br><span class="line"><span class="comment"># (copy the CLUSTER-IP for nginx-service here, e.g. 10.68.33.239) </span></span><br><span class="line"><span class="comment"># Use busybox image (sandbox within the cluster) for testing</span></span><br><span class="line">$ kubectl delete pod busybox</span><br><span class="line">$ kubectl run busybox --rm=<span class="literal">true</span> --image=busybox --restart=Never --tty -i</span><br><span class="line"><span class="comment"># In busybox container access nginx service with kube-proxy</span></span><br><span class="line">$ wget -qO - 10.68.33.239:8080</span><br><span class="line"><span class="comment"># One can also access the service directly through service name</span></span><br><span class="line">$ wget -qO - nginx-service:8080</span><br><span class="line"><span class="comment"># exit</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear-out the cluster after this demo session</span></span><br><span class="line">$ kubectl delete -f nginx-pod.yaml</span><br><span class="line">$ kubectl delete -f nginx-deployment.yaml</span><br><span class="line">$ kubectl delete -f nginx-service.yaml</span><br><span class="line">$ kubectl delete deploy kubernetes-bootcamp</span><br></pre></td></tr></table></figure><h2 id="Recommended-references"><a href="#Recommended-references" class="headerlink" title="Recommended references"></a>Recommended references</h2><p>Following YouTube links also provide some examples worth trying out:</p><p>[1] <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1LMUh1T0x6UFNwVQ==" title="https://www.youtube.com/watch?v=K1HuOLzPSpU">https://www.youtube.com/watch?v=K1HuOLzPSpU<i class="fa fa-external-link"></i></span></p><p>[2] <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj15dTNIbE9Yb0VLaw==" title="https://www.youtube.com/watch?v=yu3HlOXoEKk">https://www.youtube.com/watch?v=yu3HlOXoEKk<i class="fa fa-external-link"></i></span></p><p>[3] <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1rdlEzVlRfd0g5OA==" title="https://www.youtube.com/watch?v=kvQ3VT_wH98">https://www.youtube.com/watch?v=kvQ3VT_wH98<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post demonstrate some Kubernetes basic commends.&lt;/p&gt;
&lt;h2 id=&quot;Server-Overview&quot;&gt;&lt;a href=&quot;#Server-Overview&quot; class=&quot;headerlink&quot; title=&quot;Server Overview&quot;&gt;&lt;/a&gt;Server Overview&lt;/h2&gt;&lt;p&gt;We have setup 3 virtual machines，each has 1 cpu and 1GB memory. Details：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;Server OS&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;IP Address&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Node Type&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;CPU&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Memory&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Hostname&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;ubuntu16.04&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.56.103&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;master&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1G&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;server01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;ubuntu16.04&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.56.104&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;slave1&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1G&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;server02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;ubuntu16.04&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.56.105&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;slave2&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1G&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;server03&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;To follow this demo, root privilege is required, ask system admin (Todd) for root access.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="https://ruby-.github.io/categories/k8s/"/>
    
    
      <category term="k8s, demo" scheme="https://ruby-.github.io/tags/k8s-demo/"/>
    
  </entry>
  
</feed>
