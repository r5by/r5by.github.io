<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>k3s Dev (2) Vagrant + VirtualBox Dev.</title>
    <url>/2019/10/30/k3s-dev-2/</url>
    <content><![CDATA[<p>This post aims to help you gain a better development experience with rancher/k3s project. As an old Chinese saying implies: “A logger should always sharpen his axe before going to do his job” (工欲善其事，必先利其器). So in what follows, you may gain experiences on:</p>
<ul>
<li>Adopt k3s’s Vagrantfile to launch our virtual dev environment</li>
<li>Attach the dlv process and start debugging</li>
</ul>
<blockquote>
<p>Special thanks to Eric@RancherLabs who helps me to get to know about his graceful solution on this. Please follow this portal to admire his other works of art: <a href="https://github.com/erikwilson?tab=repositories" target="_blank" rel="noopener">大神の傳送門</a>.</p>
</blockquote>
<h2 id="1-Use-Vagrant"><a href="#1-Use-Vagrant" class="headerlink" title="1. Use Vagrant"></a>1. Use Vagrant</h2><p><a href="https://www.vagrantup.com/" target="_blank" rel="noopener">Vagrant</a> is not something new but I thought before that it was merely a VM management tool thus overlooked at this technique due to my ignorance. Until now I realize it’s extremely useful for working at an open-source project, or other similar projects that requires people from different locations to work collaboratively. This is because it offers the developer a method to “ship” his/her working environment directly to all other co-workers, in other words, this concept of “virtual dev env” enables consistency among all developers working at the same project.</p>
<blockquote>
<p>The idea of Vagrant is quite similar to Docker, for both provides certain degree of “consistency” and “isolation” in my opinion. However, they are built on top of different tech stacks (virtualization vs. container) and each has its own use-case scenario. Eric also pointed out that “Dapper is nice for building &amp; ci but kind of a pain for development”.</p>
</blockquote>
<p>To install Vagrant on MacOS, I recommend homebrew, simply issue the following commends in your terminal:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install vagrant</span><br><span class="line"><span class="comment"># or `brew cask install vagrant`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check available commands</span></span><br><span class="line">vagrant -h</span><br></pre></td></tr></table></figure>

<p>The following list shows some commonly used commends in my case:</p>
<ul>
<li><code>vagrant up</code>: spin up the boxes;</li>
<li><code>vagrant status</code>: check the status of the vagrant machine;</li>
<li><code>vagrant ssh</code>: ssh into the running machine;</li>
<li><code>vagrant halt</code>: shutdown the running boxes;</li>
<li><code>vagrant destroy</code>: delete all associated files to your configured vagrant machines.</li>
</ul>
<blockquote>
<ul>
<li>(1) Vagrant supports other VM softwares all cross platforms, for me I continue to use VirtualBox here since it’s free and I have had already certain experience with it.</li>
<li>(2) Issue the <code>destroy</code> command will remove the files for your virtual machine saved in your VirtualBox’s settings, i.e. “/yourpathto/VirtualBox VMS/xxx”.</li>
</ul>
</blockquote>
<p>After learning about the basics, let’s take a look at the Vagrantfile in the k3s project root, to see what does:</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">BOX = <span class="string">"generic/alpine310"</span></span><br><span class="line">HOME = File.dirname(__FILE_<span class="number">_</span>)</span><br><span class="line">PROJECT = File.basename(HOME)</span><br><span class="line">MOUNT_TYPE = ENV[<span class="string">'MOUNT_TYPE'</span>] <span class="params">||</span> <span class="string">"nfs"</span></span><br><span class="line">NUM_NODES = (ENV[<span class="string">'NUM_NODES'</span>] <span class="params">||</span> <span class="number">0</span>).to_i</span><br><span class="line">NODE_CPUS = (ENV[<span class="string">'NODE_CPUS'</span>] <span class="params">||</span> <span class="number">4</span>).to_i</span><br><span class="line">NODE_MEMORY = (ENV[<span class="string">'NODE_MEMORY'</span>] <span class="params">||</span> <span class="number">8192</span>).to_i</span><br><span class="line">NETWORK_PREFIX = ENV[<span class="string">'NETWORK_PREFIX'</span>] <span class="params">||</span> <span class="string">"10.135.135"</span></span><br><span class="line">VAGRANT_PROVISION = ENV[<span class="string">'VAGRANT_PROVISION'</span>] <span class="params">||</span> <span class="string">"./scripts/vagrant-provision"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Rules for /etc/sudoers to avoid password entry configuring NFS:</span></span><br><span class="line"><span class="comment"># %admin	ALL = (root) NOPASSWD: /usr/bin/sed -E -e * -ibak /etc/exports</span></span><br><span class="line"><span class="comment"># %admin	ALL = (root) NOPASSWD: /usr/bin/tee -a /etc/exports</span></span><br><span class="line"><span class="comment"># %admin	ALL = (root) NOPASSWD: /sbin/nfsd restart</span></span><br><span class="line"><span class="comment"># --- May need to add terminal to System Preferences -&gt; Security &amp; Privacy -&gt; Privacy -&gt; Full Disk Access</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Check for missing plugins</span></span><br><span class="line">required_plugins = <span class="string">%w( vagrant-alpine vagrant-timezone )</span></span><br><span class="line">plugin_installed = <span class="literal">false</span></span><br><span class="line">required_plugins.each <span class="keyword">do</span> <span class="params">|plugin|</span></span><br><span class="line">  <span class="keyword">unless</span> Vagrant.has_plugin?(plugin)</span><br><span class="line">    system <span class="string">"vagrant plugin install <span class="subst">#&#123;plugin&#125;</span>"</span></span><br><span class="line">    plugin_installed = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment"># --- If new plugins installed, restart Vagrant process</span></span><br><span class="line"><span class="keyword">if</span> plugin_installed === <span class="literal">true</span></span><br><span class="line">  exec <span class="string">"vagrant <span class="subst">#&#123;ARGV.join<span class="string">' '</span>&#125;</span>"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">provision = &lt;&lt;SCRIPT</span><br><span class="line"><span class="comment"># --- Use system gopath if available</span></span><br><span class="line">export GOPATH=<span class="comment">#&#123;ENV['GOPATH']&#125;</span></span><br><span class="line"><span class="comment"># --- Default to root user for vagrant ssh</span></span><br><span class="line">cat &lt;&lt;\\EOF &gt;<span class="regexp">/etc/profile</span>.d/root.sh</span><br><span class="line">[ $EUID -ne <span class="number">0</span> ] &amp;&amp; exec sudo -i</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># --- Set home to current directory</span></span><br><span class="line">cat &lt;&lt;\\EOF &gt;<span class="regexp">/etc/profile</span>.d/home.sh</span><br><span class="line">export HOME=<span class="string">"<span class="subst">#&#123;HOME&#125;</span>"</span> &amp;&amp; cd</span><br><span class="line">EOF</span><br><span class="line">. /etc/profile.d/home.sh</span><br><span class="line"><span class="comment"># --- Run vagrant provision script if available</span></span><br><span class="line"><span class="keyword">if</span> [ ! -x <span class="comment">#&#123;VAGRANT_PROVISION&#125; ]; then</span></span><br><span class="line">  echo <span class="string">'WARNING: Unable to execute provision script "<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>"'</span></span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line">echo <span class="string">"running '<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>'..."</span> &amp;&amp; \</span><br><span class="line">  <span class="comment">#&#123;VAGRANT_PROVISION&#125; &amp;&amp; \</span></span><br><span class="line">  echo <span class="string">"finished '<span class="subst">#&#123;VAGRANT_PROVISION&#125;</span>'!"</span></span><br><span class="line">SCRIPT</span><br><span class="line"></span><br><span class="line">Vagrant.configure(<span class="string">"2"</span>) <span class="keyword">do</span> <span class="params">|config|</span></span><br><span class="line">  config.vm.provider <span class="string">"virtualbox"</span> <span class="keyword">do</span> <span class="params">|v|</span></span><br><span class="line">    v.cpus = NODE_CPUS</span><br><span class="line">    v.memory = NODE_MEMORY</span><br><span class="line">    v.customize [<span class="string">"modifyvm"</span>, <span class="symbol">:id</span>, <span class="string">"--audio"</span>, <span class="string">"none"</span>]</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  config.vm.box = BOX</span><br><span class="line">  config.vm.hostname = PROJECT</span><br><span class="line">  config.vm.synced_folder <span class="string">"."</span>, HOME, <span class="symbol">type:</span> MOUNT_TYPE</span><br><span class="line">  config.vm.provision <span class="string">"shell"</span>, <span class="symbol">inline:</span> provision</span><br><span class="line">  config.timezone.value = <span class="symbol">:host</span></span><br><span class="line"></span><br><span class="line">  config.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"<span class="subst">#&#123;NETWORK_PREFIX&#125;</span>.100"</span> <span class="keyword">if</span> NUM_NODES==<span class="number">0</span></span><br><span class="line"></span><br><span class="line">  (<span class="number">1</span>..NUM_NODES).each <span class="keyword">do</span> <span class="params">|i|</span></span><br><span class="line">    config.vm.define <span class="string">".<span class="subst">#&#123;i&#125;</span>"</span> <span class="keyword">do</span> <span class="params">|node|</span></span><br><span class="line">      node.vm.network <span class="string">"private_network"</span>, <span class="symbol">ip:</span> <span class="string">"<span class="subst">#&#123;NETWORK_PREFIX&#125;</span>.<span class="subst">#&#123;<span class="number">100</span>+i&#125;</span>"</span></span><br><span class="line">      node.vm.hostname = <span class="string">"<span class="subst">#&#123;PROJECT&#125;</span>-<span class="subst">#&#123;i&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<p>As we can see, mainly what it does is to pull the base box image and then prepare it by evoking the provision script and setting up the network. In particular our case, we need to:</p>
<ul>
<li><p>(1) Set up the environment variables to bring up vagrant boxes as we want. For example:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># how many node do we want</span></span><br><span class="line"><span class="built_in">export</span> NUM_NODES=2</span><br><span class="line"></span><br><span class="line"><span class="comment"># how many cpus we have for each node</span></span><br><span class="line"><span class="built_in">export</span> NODE_CPUS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Also can be customized include the network, memory, etc.</span></span><br><span class="line"><span class="comment"># Finally launch the boxes with the above configurations</span></span><br><span class="line">vagrant up</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the running machines</span></span><br><span class="line">vagrant status</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connect to one of the above machines</span></span><br><span class="line">vagrant ssh .1</span><br><span class="line"></span><br><span class="line"><span class="comment"># After inspecting the vagrant, close and destroy it</span></span><br><span class="line"><span class="comment"># or stop by `vagrant halt`</span></span><br><span class="line">vagrant destroy</span><br></pre></td></tr></table></figure>
</li>
<li><p>(2) The actually box image that has pulled down and configured is saved at: <code>$HOME/.vagrant.d/boxes</code>.</p>
</li>
<li><p>(3) To see what vagrant actually does in the <code>ssh</code> procedure, open another terminal then issue <code>ps aux | grep ssh</code> to verify it, then connect to that node again in another terminal by using that command:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh vagrant@127.0.0.1 -p 2222 -o LogLevel=FATAL -o Compression=yes -o DSAAuthentication=yes -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /yourpathto/k3s/.vagrant/machines/.1/virtualbox/private_key</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="2-Debugging"><a href="#2-Debugging" class="headerlink" title="2. Debugging"></a>2. Debugging</h2><p>After admiring vagrant, destroy these existing boxes and re-prepare them for enabling delve debugger by adding just one line to the <code>scripts/vagrant-provision</code> file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -ve</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> $(dirname <span class="variable">$0</span>)/..</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line">mkdir -p <span class="variable">$&#123;GOPATH&#125;</span>/bin</span><br><span class="line">mkdir -p /go</span><br><span class="line">ln -s <span class="variable">$GOPATH</span>/bin /go/bin</span><br><span class="line">sed <span class="string">':a;N;$!ba;s/\\\n/ /g'</span> &lt;Dockerfile.dapper | grep <span class="string">'^RUN '</span> | sed -e <span class="string">'s/^RUN //'</span> &gt;/tmp/docker-run</span><br><span class="line"><span class="built_in">export</span> BINDIR=/go/bin</span><br><span class="line"><span class="built_in">export</span> GOPATH=/go</span><br><span class="line"><span class="built_in">export</span> HOME=/tmp &amp;&amp; <span class="built_in">cd</span></span><br><span class="line">. /tmp/docker-run</span><br><span class="line"><span class="built_in">cd</span> /go</span><br><span class="line">go get github.com/rancher/trash</span><br><span class="line"><span class="comment"># --- Add one line here to enable delve           &lt;==</span></span><br><span class="line">go get -u github.com/go-delve/delve/cmd/dlv</span><br><span class="line">rm -rf /go</span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Now ssh into the virtual machine and start debugging as we already learned in the previous post:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build from the source</span></span><br><span class="line">./scripts/download &amp;&amp; ./scripts/build &amp;&amp; ./scripts/package-cli</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch dlv</span></span><br><span class="line">dlv --listen=:2345 --headless=<span class="literal">true</span> --api-version=2 --accept-multiclient <span class="built_in">exec</span> dist/artifacts/k3s -- --debug server</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>Note</strong>: Use <code>netstat</code> you will notice some nfs related daemons, these rpc procedures are critical to maintain the consistency between the host’s source files and those to be built in virtual dev boxes. Also, to kill the dlv debugger simply kill the process from a different terminal. The most enhanced experience (at least for me) is that any modification on the source code in my host will be directly synchronized to the virtual box side via the mount, bravo!</p>
</blockquote>
<h2 id="3-Summary"><a href="#3-Summary" class="headerlink" title="3. Summary"></a>3. Summary</h2><p>In this post we have used Vagrant and Virtualbox to set up our dev environment. In my following posts, we’ll continue to dig deeper into k3s’ source code and learn more about it soon.</p>
]]></content>
      <categories>
        <category>edge computing</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k3s Dev (1) Environment Setup</title>
    <url>/2019/10/10/k3s-dev-1/</url>
    <content><![CDATA[<p>This post shows in steps how to set up a dev environment for <strong>k3s</strong> project.</p>
<h2 id="Step-1-Preparation"><a href="#Step-1-Preparation" class="headerlink" title="Step 1. Preparation"></a>Step 1. Preparation</h2><p>The following list shows my local development environment setup:</p>
<p>For code review &amp; build purpose:</p>
<ul>
<li>OS: Mac OS </li>
<li>Docker: 18.06.1-ce</li>
<li>JetBrains GoLand IDE</li>
<li>golang: 1.12.7</li>
</ul>
<p>For testing deploy purpose:</p>
<ul>
<li>VirtualBox</li>
<li>Ubuntu 18.04 Server</li>
</ul>
<blockquote>
<p>Alternatively, one could use EC2 instances as testing deploy environment, in which case may directly to go step 2.</p>
</blockquote>
<h3 id="1-1-VirtualBox-Setup"><a href="#1-1-VirtualBox-Setup" class="headerlink" title="1.1 VirtualBox Setup"></a>1.1 VirtualBox Setup</h3><p>Download Ubuntu 18.04 iso from <a href="http://releases.ubuntu.com/18.04/" target="_blank" rel="noopener">official source</a> and install it on VirtualBox. Configure the network interface as following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check all available network adapters</span></span><br><span class="line">ifconfig -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then Use netplan to config all interfaces</span></span><br><span class="line">sudo vi /etc/netplan/50-cloud-init.yaml</span><br></pre></td></tr></table></figure>

<p>Change the content accordingly (192.168.56.1 is my gateway setting, change it according to your own VirtualBox network setting):</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line"><span class="attr">    ethernets:</span></span><br><span class="line"><span class="attr">        enp0s3:</span></span><br><span class="line"><span class="attr">            addresses:</span> <span class="string">[]</span></span><br><span class="line"><span class="attr">            dhcp4:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            dhcp6:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">            optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            nameservers:</span></span><br><span class="line"><span class="attr">               addresses:</span> <span class="string">[8.8.8.8]</span></span><br><span class="line"><span class="attr">        enp0s8:</span></span><br><span class="line"><span class="attr">             addresses:</span> <span class="string">[192.168.56.10/24]</span></span><br><span class="line"><span class="attr">             dhcp4:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">             dhcp6:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">             optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">             routes:</span></span><br><span class="line"><span class="attr">                - to:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span><span class="string">/24</span></span><br><span class="line"><span class="attr">                  via:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span></span><br><span class="line"><span class="attr">    version:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>Save and apply the changes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo netplan apply</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test connectivity</span></span><br><span class="line">ping 192.168.56.101 <span class="comment"># another virtual machine</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test installing k3s</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> sh -</span><br></pre></td></tr></table></figure>

<h3 id="1-2-Golang-Environment"><a href="#1-2-Golang-Environment" class="headerlink" title="1.2 Golang Environment"></a>1.2 Golang Environment</h3><p>For Mac OS users, I recommend use <code>homebrew</code> to manage go environment. However, go has its own version manager called GVM, refer <a href="https://github.com/moovweb/gvm" target="_blank" rel="noopener">here</a> for detailed information. To use earlier version of go, add following to your <code>.bashrc</code> file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># homebrew go version config </span></span><br><span class="line"><span class="function"><span class="title">goconfig</span></span>() &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></span><br><span class="line">        <span class="string">"1.12"</span>)</span><br><span class="line">            brew switch go 1.12.7</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.12.7/libexec</span><br><span class="line">            ;;</span><br><span class="line">        <span class="string">"1.13"</span>)</span><br><span class="line">            brew switch go 1.13</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.13/libexec</span><br><span class="line">            ;;</span><br><span class="line">        *)</span><br><span class="line">            brew switch go 1.12.7</span><br><span class="line">            <span class="built_in">export</span> GOROOT=/usr/<span class="built_in">local</span>/Cellar/go/1.12.7/libexec</span><br><span class="line">            ;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Installed Go path</span></span><br><span class="line">    <span class="built_in">export</span> GOPATH=<span class="variable">$HOME</span>/go</span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:GOROOT/bin</span><br><span class="line">    <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:$(go env GOPATH)/bin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>To use simply pass the version selected. For example, to change to go version 12:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Switch to go version 12</span></span><br><span class="line">goconfig 1.12</span><br></pre></td></tr></table></figure>

<h3 id="1-3-Source-Code-Download"><a href="#1-3-Source-Code-Download" class="headerlink" title="1.3 Source Code Download"></a>1.3 Source Code Download</h3><p>With go environment set up, now we go to its workspace to pull the source from github (I have already forked k3s project to my own github account, if that’s not the case for you, simply pull the source from <a href="https://github.com/rancher/k3s" target="_blank" rel="noopener">rancher</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /Users/yourname/go/src/github.com/yourgithubacct</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download my forked project to local</span></span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/ruby-/k3s.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add rancher remote source</span></span><br><span class="line">git remote add rancher https://github.com/rancher/k3s.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># now there should be two remote sources: origin (my own) and rancher's</span></span><br><span class="line">git remote</span><br><span class="line"></span><br><span class="line"><span class="comment"># fetch from rancher's master branch and merge to local (shouldn't be any change yet)</span></span><br><span class="line">git fetch rancher</span><br><span class="line">git merge rancher/master</span><br></pre></td></tr></table></figure>

<h2 id="Step-2-Build-amp-Test-Launch"><a href="#Step-2-Build-amp-Test-Launch" class="headerlink" title="Step 2. Build &amp; Test Launch"></a>Step 2. Build &amp; Test Launch</h2><p>Open the k3s folder in GoLand IDE, on Mac OS, press <code>⌘ + ⇧ + f</code> to search in path for <em>“k3s is up and running”</em>, open the <em>pkg/cli/server/server.go</em> file and add following statement:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">run</span><span class="params">(app *cli.Context, cfg *cmds.Server)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    ctx := signals.SetupSignalHandler(context.Background())</span><br><span class="line">    certs, err := server.StartServer(ctx, &amp;serverConfig)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    logrus.Info(<span class="string">"k3s is up and running"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Add a new logging info here</span></span><br><span class="line">    logrus.Info(<span class="string">"==================&gt; Fog is just clouds that have fell down! &lt;=================="</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> notifySocket != <span class="string">""</span> &#123;</span><br><span class="line">    	os.Setenv(<span class="string">"NOTIFY_SOCKET"</span>, notifySocket)</span><br><span class="line">        systemd.SdNotify(<span class="literal">true</span>, <span class="string">"READY=1\n"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Now open the terminal within GoLand IDE, then issue <code>make</code> command and wait for it’s pulling required docker images and build the project for us. After the building procedure, there should be <strong>hyperkube</strong>, <strong>k3s</strong> produced in the <em>dist/artifacts/</em> folder. Simply copy <strong>k3s</strong> to your deploy environment for testing purpose (for me only need to copy to virtualbox ubuntu server that has been setup):</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp dist/artifacts/k3s main@192.168.56.10:~/k3s/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start k3s server and verify the added logged info</span></span><br><span class="line">k3s server &gt; server.log 2&gt;&amp;1 &amp;</span><br><span class="line">vi server.log</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Note that:</p>
<ul>
<li>(1) Remember to <code>chmod 777 k3s_install_path</code> the install path for <code>scp</code> to be able to upload k3s executable; </li>
<li>(2) k3s doesn’t support cross compile at current release, refer to <a href="https://github.com/rancher/k3s/issues/530" target="_blank" rel="noopener">issue#530</a> for detailed discussion regarding this;</li>
<li>(3) There are two options for developers to build k3s from source. We are adapting docker method with <a href="https://github.com/rancher/dapper" target="_blank" rel="noopener">rancher/dapper</a> wrapper in this post. However for those who are working on Linux, may alternatively download the dependencies and directly build the source with <code>go build</code> commend described <a href="https://rancher.com/docs/k3s/latest/en/building/" target="_blank" rel="noopener">here</a>; unfortunately this compile option is not available for Mac OS users at current release, refer to <a href="https://github.com/rancher/k3s/issues/273" target="_blank" rel="noopener">issue#273</a> for detailed discussion regarding this.</li>
<li>(4) For Mac OS users, docker tends to accumulate its data file (store at <code>/Users/yourname/Library/Containers/com.docker.docker/Data/vms/0/Docker.qcow2</code>) which contains its self os image and downloaded containers. Build <strong>k3s</strong> project may consume ~10GB space on disk and it keeps growing. The way to get around this is to use docker built-in “reset” tab (Also note that <code>docker system prune</code> won’t save you from this pitfall).</li>
</ul>
</blockquote>
<h2 id="Step-3-Breakpoint-amp-debugging"><a href="#Step-3-Breakpoint-amp-debugging" class="headerlink" title="Step 3. Breakpoint &amp; debugging"></a>Step 3. Breakpoint &amp; debugging</h2><p>It is essential for us engineers to be able to set up breakpoint and debug the code in software development. In this section we’ll try to connect to remotely deployed <strong>k3s</strong> executable and use <a href="https://github.com/go-delve/delve" target="_blank" rel="noopener">delve</a> tool to navigate through the debugging process. </p>
<h3 id="3-1-Install-go-and-dlv-in-deploy-env"><a href="#3-1-Install-go-and-dlv-in-deploy-env" class="headerlink" title="3.1 Install go and dlv in deploy env"></a>3.1 Install go and dlv in deploy env</h3><p>To install and set up go environment and dlv debugger on Ubuntu 18.04 server VM we created earlier, simply follow these two guides:</p>
<ul>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-go-on-ubuntu-18-04" target="_blank" rel="noopener">How To Install Go on Ubuntu 18.04</a></li>
<li><a href="https://github.com/go-delve/delve/blob/master/Documentation/installation/linux/install.md" target="_blank" rel="noopener">dlv - Installation on Linux</a></li>
</ul>
<p>To get familiar with <code>dlv</code> basic workflows and combine it with JetBrain GoLand IDE, I recommend to follow these practices:</p>
<ul>
<li><a href="https://www.jamessturtevant.com/posts/Using-the-Go-Delve-Debugger-from-the-command-line/" target="_blank" rel="noopener">Using the Go Delve Debugger from the command line</a></li>
<li><a href="https://blog.jetbrains.com/go/2019/02/06/debugging-with-goland-getting-started/" target="_blank" rel="noopener">Debugging with GoLand – Getting Started</a></li>
</ul>
<p>Now let us go back to our <strong>k3s</strong> project, find and modify the following lines in <code>scripts/build.sh</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">VERSIONFLAGS=<span class="string">"</span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/pkg/version.Version=<span class="variable">$VERSION</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/pkg/version.GitCommit=<span class="variable">$&#123;COMMIT:0:8&#125;</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CONTAINERD</span>/version.Version=<span class="variable">$VERSION_CONTAINERD</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CONTAINERD</span>/version.Package=<span class="variable">$PKG_RANCHER_CONTAINERD</span></span></span><br><span class="line"><span class="string">    -X <span class="variable">$PKG</span>/vendor/<span class="variable">$PKG_CRICTL</span>/pkg/version.Version=<span class="variable">$VERSION_CRICTL</span>"</span></span><br><span class="line"><span class="comment"># Comment out LDFLAGS (link) for debug: "-w"= wipe out debug info ;; "-s"= remove symbol table</span></span><br><span class="line"><span class="comment"># LDFLAGS="</span></span><br><span class="line"><span class="comment">#    -w -s"</span></span><br><span class="line">LDFLAGS=<span class="string">""</span></span><br><span class="line">STATIC=<span class="string">"</span></span><br><span class="line"><span class="string">    -extldflags '-static'</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Go to <code>scripts/package-cli.sh</code> file and change the following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">go generate</span><br><span class="line">LDFLAGS=<span class="string">"</span></span><br><span class="line"><span class="string">    -X github.com/rancher/k3s/pkg/version.Version=<span class="variable">$VERSION</span></span></span><br><span class="line"><span class="string">    -X github.com/rancher/k3s/pkg/version.GitCommit=<span class="variable">$&#123;COMMIT:0:8&#125;</span></span></span><br><span class="line"><span class="string">    -w -s</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line">STATIC=<span class="string">"-extldflags '-static'"</span></span><br><span class="line"><span class="comment"># CGO_ENABLED=0 go build -ldflags "$LDFLAGS $STATIC" -o $&#123;CMD_NAME&#125; ./cmd/k3s/main.go</span></span><br><span class="line"><span class="comment"># Add gcflags to enable dlv</span></span><br><span class="line">CGO_ENABLED=0 go build -gcflags <span class="string">"all=-N -l"</span> -ldflags <span class="string">"<span class="variable">$STATIC</span>"</span> -o <span class="variable">$&#123;CMD_NAME&#125;</span> ./cmd/k3s/main.go</span><br></pre></td></tr></table></figure>

<p>Commit the file changes to our local git repo and rebuild the project, then upload the new <strong>k3s</strong> executable to our local VM:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Commit local changes</span></span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">"enable dlv"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rebuild project</span></span><br><span class="line">make</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload to deploy</span></span><br><span class="line">scp dist/artifacts/k3s main@192.168.56.10:~/k3s/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch with dlv: note `dlv exec xxx -- paras` means passing the parameters after "--" to our executable instead of dlv</span></span><br><span class="line">dlv --listen=:2345 --headless=<span class="literal">true</span> --api-version=2 --accept-multiclient <span class="built_in">exec</span> /home/main/k3s/k3s -- --debug server</span><br></pre></td></tr></table></figure>

<p>Setup a new go remote debug run/debug configuration in GoLand IDE which looks like this:<br><img src="https://drive.google.com/uc?export=view&id=1VTjvnD_Fi3o1kGt3hc_o-IyeubWb5bTX" alt="image"></p>
<p>Toggle some breakpoint at main entry then click <strong>debug</strong> button to start debugging.<br><img src="https://drive.google.com/uc?export=view&id=1Yf24i0WFEMJR-mnvyOg79X_xcnqx_06t" alt="image"></p>
<blockquote>
<p>Some of my questions:</p>
<ul>
<li>(1) <del>Use the <code>make</code> method to build the whole project takes long time (3-5 min for me). I wonder if there’s some “quick” and “convenient” way to speedup this procedure, e.g. build only parts that have been changed? Please leave comments below if you know the answer to this.</del><br>(Updated) Please refer my following post for a better way to separate source code edit &amp; build/test environment here at <a href="/2019/10/30/k3s-dev-2/" title="k3s Dev (2) Vagrant + VirtualBox Dev.">k3s Dev (2) Vagrant + VirtualBox Dev.</a></li>
<li>(2) The latest stable version of GoLand(2019.2) leave the channel open after the debugging procedure has been stopped within IDE. Simple kill the dlv by PID to get around this issue.</li>
<li>(3) Run <code>k3s</code> command with generate a bunch of code in <code>/var/lib/rancher/k3s/</code>, these run time generated binaries and configurations contain most important components including <code>k3s-server</code> and <code>k3s-agent</code>. Details about these we’ll explore also in next my post.</li>
</ul>
</blockquote>
<h2 id="Summery"><a href="#Summery" class="headerlink" title="Summery"></a>Summery</h2><p>Learning k3s from source is important for us to contribute to its community. In my next post, I’ll try to investigate its scheduling mechanism and explain the related major workflow.</p>
]]></content>
      <categories>
        <category>edge computing</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k3s Basics</title>
    <url>/2019/09/29/k3s-basics/</url>
    <content><![CDATA[<p>This post explains my first impression with <strong>k3s</strong>.</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In order to work on k3s project, one needs to learn to use it at first. This post summarize the main steps of deploying <a href="https://github.com/rancher/k3s" target="_blank" rel="noopener">k3s</a> in our local lab environment and also on AWS EC2.</p>
<h2 id="Local-k3s-cluster-Setup"><a href="#Local-k3s-cluster-Setup" class="headerlink" title="Local k3s cluster Setup"></a>Local k3s cluster Setup</h2><p>The basic hardware setup for this demo is as the same as the last post on k8s demo (3 VMs), feel free to apply the following methods with the latest released k3s on your own Raspbian nodes, it should work as we also tested it. However, the latest released version (0.9.1 at the time of this post) of k3s has some ca-related issues for an agent to join the master node (if a full ca-enabled Kubernetes cluster has been configured on these nodes), thus an earlier release version (0.2.0) was used for this section.</p>
<h3 id="Step-1-Stop-the-previously-installed-k8s-related-services-optional"><a href="#Step-1-Stop-the-previously-installed-k8s-related-services-optional" class="headerlink" title="Step 1: Stop the previously installed k8s related services (optional)"></a>Step 1: Stop the previously installed k8s related services (optional)</h3><p>On master node:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service kube-calico stop</span><br><span class="line">service kube-scheduler stop</span><br><span class="line">service kube-controller-manager stop</span><br><span class="line">service kube-apiserver stop</span><br><span class="line">service etcd stop &amp;&amp; rm -fr /var/lib/etcd/*</span><br></pre></td></tr></table></figure>

<p>On worker nodes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">service kubelet stop &amp;&amp; rm -fr /var/lib/kubelet/*</span><br><span class="line">service kube-proxy stop &amp;&amp; rm -fr /var/lib/kube-proxy/*</span><br><span class="line">service kube-calico stop</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Here I simply turn k8s off to avoid potential conflicts between k8s and k3s deployment. However this doesn’t prevent the ca issue for latest k3s releases (&gt;v0.2.0) to work with previously installed k8s environment.</p>
</blockquote>
<h3 id="Step-2-Deploy-k3s-on-each-node"><a href="#Step-2-Deploy-k3s-on-each-node" class="headerlink" title="Step 2: Deploy k3s on each node"></a>Step 2: Deploy k3s on each node</h3><p>To install k3s is quite simple, we will use the following commands install and start k3s on master and two worker machines separately. </p>
<p>On master node:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install k3s with rancher script</span></span><br><span class="line"><span class="comment"># The script download the binaries, pull the images (containerd) and enable/start the k3s-related systemctl services</span></span><br><span class="line">mkdir k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># With INSTALL_K3S_EXEC="--disable-agent" option, one may launch k3s server on the node without an agent (which may cause some issues with current release of k3s)</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> INSTALL_K3S_VERSION=<span class="string">"v0.2.0"</span>  sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the k3s system services are listening on their ports</span></span><br><span class="line">netstat -nltp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the k3s cluster status</span></span><br><span class="line">systemctl status k3s</span><br></pre></td></tr></table></figure>

<blockquote>
<p>On master node, one should see services and their port numbers: <em>k3s : 6443/6444, 10251/10252</em></p>
</blockquote>
<p>After installation, the k3s binary folder looks like this:</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── crictl -&gt; k3s</span><br><span class="line">├── k3s</span><br><span class="line">├── k3s-killall.sh</span><br><span class="line">└── k3s-uninstall.sh</span><br></pre></td></tr></table></figure>

<blockquote>
<p>With out the INSTALL_K3S_BIN_DIR option, k3s will be installed at /usr/local/bin</p>
</blockquote>
<p>Now, in order to join new workers to this master node, one needs first grab the token on that (master) node:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example output: K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bb</span></span><br><span class="line">cat /var/lib/rancher/k3s/server/node-token</span><br></pre></td></tr></table></figure>

<p>On worker nodes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># download &amp; active the k3s-agent service</span></span><br><span class="line">curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=<span class="string">"/home/main/k3s"</span> INSTALL_K3S_VERSION=<span class="string">"v0.2.0"</span> K3S_TOKEN=<span class="string">"K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bb"</span> K3S_URL=<span class="string">"https://192.168.56.103:6443"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the service status</span></span><br><span class="line">systemctl status k3s-agent</span><br></pre></td></tr></table></figure>

<p>The installation of k3s agent on worker nodes looks like this:</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── crictl -&gt; k3s</span><br><span class="line">├── k3s</span><br><span class="line">├── k3s-agent-uninstall.sh</span><br><span class="line">└── k3s-killall.sh</span><br></pre></td></tr></table></figure>

<blockquote>
<p>On worker node, k3s has services and ports: <em>k3s : 42323, containerd : 10010</em></p>
</blockquote>
<p>Now on the master node, one should be able to verify the newly added cluster resources:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Now verify the newly added worker in the cluster</span></span><br><span class="line">k3s kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">## Do some deployment here...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Clean-up</span></span><br><span class="line"><span class="comment"># Kill k3s services after inspection (on each node)</span></span><br><span class="line">k3s-killall.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uninstall k3s on master</span></span><br><span class="line">k3s-uninstall.sh</span><br><span class="line"><span class="comment"># Uninstall k3s on worker nodes</span></span><br><span class="line">k3s-agent-uninstall.sh</span><br></pre></td></tr></table></figure>

<h2 id="Install-k3s-manually-on-EC2-instances"><a href="#Install-k3s-manually-on-EC2-instances" class="headerlink" title="Install k3s manually on EC2 instances"></a>Install k3s manually on EC2 instances</h2><p>This section explains how to manually download k3s binaries from rancher’s official release and cluster-up. Here we use AWS EC2 service as following configuration:</p>
<table>
<thead>
<tr>
<th align="center">Instance OS</th>
<th align="center">Arch</th>
<th align="center">IP (internal)</th>
<th align="center">Instance Type</th>
<th align="center">vCPU</th>
<th align="center">Memory</th>
<th align="center">Node Role</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Ubuntu Server 18.04 LTS (HVM), SSD Volume Type</td>
<td align="center">amd64(x86_64)</td>
<td align="center">172.31.46.70</td>
<td align="center">t2.medium</td>
<td align="center">2</td>
<td align="center">4GiB</td>
<td align="center">master</td>
</tr>
<tr>
<td align="center">Amazon Linux 2 AMI (HVM), SSD Volume Type</td>
<td align="center">arm64(aarch64)</td>
<td align="center">172.31.36.129</td>
<td align="center">a1.medium</td>
<td align="center">1</td>
<td align="center">2GiB</td>
<td align="center">worker</td>
</tr>
</tbody></table>
<blockquote>
<p>Remember to allow all traffic from anywhere in your security group setting.</p>
</blockquote>
<p>First, let us <code>ssh</code> to each running instance and prepare the k3s executable</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Change to your own pem key file and instance address:</span></span><br><span class="line">ssh -i ~/.ssh/your-key.pem ubuntu@ec2-x-x-x-x.region.compute.amazonaws.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare download folder</span></span><br><span class="line">mkdir k3s</span><br><span class="line"><span class="built_in">cd</span> k3s</span><br><span class="line"></span><br><span class="line"><span class="comment">## Download the desired release version from: https://github.com/rancher/k3s/releases?after=v0.10.0-alpha1</span></span><br><span class="line"><span class="comment"># On master (x86_64)</span></span><br><span class="line">wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s</span><br><span class="line"><span class="comment"># On worker (arm)</span></span><br><span class="line">wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s-arm64</span><br><span class="line">mv k3s-arm64 k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add exec mode</span></span><br><span class="line">chmod +x k3s</span><br></pre></td></tr></table></figure>

<p>On master node:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Start k3s server</span></span><br><span class="line">./k3s server &gt; server.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get token copy-and-paste the output to your worker:</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export node_token=<span class="variable">$(cat /var/lib/rancher/k3s/server/node-token)</span>"</span></span><br></pre></td></tr></table></figure>

<p>On worker node:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copy-and-paste token from master here</span></span><br><span class="line"><span class="built_in">export</span> node_token=...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start agent, pass server url and token</span></span><br><span class="line">./k3s agent --server https://172.31.46.70:6443 --token <span class="string">"<span class="variable">$node_token</span>"</span> &gt;&amp; k3s-agent.log &amp;</span><br></pre></td></tr></table></figure>

<p>After a little while, check the cluster info with <code>k3s kubectl</code> command described in last section on the master node.</p>
<blockquote>
<p>Simply kill the PID to stop k3s or k3s-agent in the demo to shut down the cluster after inspection.</p>
</blockquote>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Ranger k3s is much smaller and easier to deploy compared to Kubernetes, and it requires less effort and resources to set up. In next post, we will discuss how to set-up a development environment on k3s and dive deeper to learn k3s from its source code.</p>
]]></content>
      <categories>
        <category>edge computing</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s Basics</title>
    <url>/2019/09/17/k8s-basics/</url>
    <content><![CDATA[<p>This post demonstrate some Kubernetes basic commends.</p>
<h2 id="Server-Overview"><a href="#Server-Overview" class="headerlink" title="Server Overview"></a>Server Overview</h2><p>We have setup 3 virtual machines，each has 1 cpu and 1GB memory. Details：</p>
<table>
<thead>
<tr>
<th align="center">Server OS</th>
<th align="center">IP Address</th>
<th align="center">Node Type</th>
<th align="center">CPU</th>
<th align="center">Memory</th>
<th align="center">Hostname</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ubuntu16.04</td>
<td align="center">192.168.56.103</td>
<td align="center">master</td>
<td align="center">1</td>
<td align="center">1G</td>
<td align="center">server01</td>
</tr>
<tr>
<td align="center">ubuntu16.04</td>
<td align="center">192.168.56.104</td>
<td align="center">slave1</td>
<td align="center">1</td>
<td align="center">1G</td>
<td align="center">server02</td>
</tr>
<tr>
<td align="center">ubuntu16.04</td>
<td align="center">192.168.56.105</td>
<td align="center">slave2</td>
<td align="center">1</td>
<td align="center">1G</td>
<td align="center">server03</td>
</tr>
</tbody></table>
<blockquote>
<p>To follow this demo, root privilege is required, ask system admin (Todd) for root access.</p>
</blockquote>
<a id="more"></a>

<h2 id="Startup-all-nodes"><a href="#Startup-all-nodes" class="headerlink" title="Startup all nodes"></a>Startup all nodes</h2><p>Login to the system then start virtualBox and each virtual machine.</p>
<h3 id="Launch-VirtualBox"><a href="#Launch-VirtualBox" class="headerlink" title="Launch VirtualBox"></a>Launch VirtualBox</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get root privilege</span></span><br><span class="line">$ sudo -s</span><br><span class="line"><span class="comment"># Then start master, slave1 and slave2 virtual machines </span></span><br><span class="line">$ virtualbox</span><br></pre></td></tr></table></figure>

<h3 id="Login-to-each-node"><a href="#Login-to-each-node" class="headerlink" title="Login to each node"></a>Login to each node</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ Login as: main</span><br><span class="line"><span class="comment"># passwd: 000000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># And get root priviledge on each node</span></span><br><span class="line">$ sudo -s</span><br></pre></td></tr></table></figure>

<h3 id="Check-system-information-on-each-node"><a href="#Check-system-information-on-each-node" class="headerlink" title="Check system information on each node"></a>Check system information on each node</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check ip address</span></span><br><span class="line">$ ifconfig</span><br><span class="line"><span class="comment"># check running containers</span></span><br><span class="line">$ docker ps</span><br><span class="line"><span class="comment"># check services &amp; ports</span></span><br><span class="line">$ netstat -nltp</span><br></pre></td></tr></table></figure>

<blockquote>
<p>On master node, one should see services and their port numbers: <em>kube-apiserver : 6443/8080, etcd : 2379/2380, kube-scheduler : 10251, kube-controller : 10252, calico-felix : 9099</em></p>
</blockquote>
<blockquote>
<p>On worker nodes, one should see services and their port numbers: <em>kubelet : 4194/10248/10250/10255, kube-proxy:10249/10256, calico-felix : 9099</em></p>
</blockquote>
<h2 id="Commonly-used-commands"><a href="#Commonly-used-commands" class="headerlink" title="Commonly used commands"></a>Commonly used commands</h2><h3 id="Use-calico-to-check-the-network-status-on-each-node"><a href="#Use-calico-to-check-the-network-status-on-each-node" class="headerlink" title="Use calico to check the network status on each node"></a>Use calico to check the network status on each node</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ calicoctl node status</span><br></pre></td></tr></table></figure>

<blockquote>
<p>At each node, it should be able to see the other two nodes’ ip addresses in the cluster.</p>
</blockquote>
<h3 id="Use-kubectl-on-master-node-to-verify-the-cluster-resources-deloyment-nodes-pods-services-etc"><a href="#Use-kubectl-on-master-node-to-verify-the-cluster-resources-deloyment-nodes-pods-services-etc" class="headerlink" title="Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)"></a>Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check server/client version</span></span><br><span class="line">$ kubectl version</span><br><span class="line"><span class="comment"># get workers</span></span><br><span class="line">$ kubectl get node</span><br><span class="line"><span class="comment"># get pods</span></span><br><span class="line">$ kubectl get pods</span><br><span class="line"><span class="comment"># get deployment</span></span><br><span class="line">$ kubectl get deploy</span><br><span class="line"><span class="comment"># get services</span></span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure>

<h3 id="More-kubectl-commands"><a href="#More-kubectl-commands" class="headerlink" title="More kubectl commands"></a>More kubectl commands</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl run kubernetes-bootcamp --image=jocatalin/kubernetes-bootcamp:v1 --port=8080</span><br><span class="line"><span class="comment"># check deploy/pods again</span></span><br><span class="line">$ kubectl get deploy</span><br><span class="line">$ kubectl get pods</span><br><span class="line"><span class="comment"># i.e. NAME: kubernetes-bootcamp-6b7849c495-p7dsw</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then check log of the pod</span></span><br><span class="line">$ kubectl logs kubernetes-bootcamp-6b7849c495-p7dsw -f</span><br><span class="line"><span class="comment"># (ctrl-c out the following log)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># describe pod</span></span><br><span class="line">$ kubectl describe pods kubernetes-bootcamp-6b7849c495-p7dsw</span><br><span class="line"><span class="comment"># (Find the Mounts:/var/run/secrets/kubernetes.io/serviceaccount in the description)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enter the running pod and verify the above path</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it kubernetes-bootcamp-6b7849c495-p7dsw bash</span><br><span class="line"><span class="comment"># Find out the certificate files in that path</span></span><br><span class="line">$ ls -l /var/run/secrets/kubernetes.io/serviceaccount </span><br><span class="line"><span class="comment"># Exit the pod</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"><span class="comment"># These ca files are actually associate with the ca account, check that info by:</span></span><br><span class="line">$ kubectl get sa -o yaml</span><br><span class="line"><span class="comment"># can also output other pattern, e.g. json</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check the "secret"</span></span><br><span class="line">$ kubectl get secrets -o yaml</span><br><span class="line"><span class="comment"># (The content of this secret has 3 sections that are mounted as three files in each pod as we see in above)</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>The “secret” are mounted to each created pod as files located in /var/run/secrets/… so that each pod can connect with api-server with https requests.</p>
</blockquote>
<h3 id="Use-‘apply’-or-‘create’-with-yaml-files"><a href="#Use-‘apply’-or-‘create’-with-yaml-files" class="headerlink" title="Use ‘apply’ or ‘create’ with yaml files"></a>Use ‘apply’ or ‘create’ with yaml files</h3><p>Kubectl ‘apply’ command is similar to ‘create’ command, but has rich properties.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> services</span><br><span class="line"><span class="comment"># Create a nginx pod and verify</span></span><br><span class="line">$ kubectl apply -f nginx-pod.yaml</span><br><span class="line">$ kubectl describe pod nginx</span><br><span class="line"><span class="comment"># (version here used is 1.7.9)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vi nginx-pod.yaml and change image:nginx:1.7.9 -&gt; image:nginx:1.13, re-apply the yaml file</span></span><br><span class="line">$ kubectl apply -f nginx-pod.yaml</span><br><span class="line"><span class="comment"># (version here is now 1.13)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to change version of running image is to use 'set' command</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image pods nginx nginx=nginx:1.7.9</span><br><span class="line"><span class="comment"># (reset the version to 1.7.9)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 'apply' command can be also used to create other resources</span></span><br><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line">$ kubectl apply -f nginx-service.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the service is runing</span></span><br><span class="line">$ curl 192.168.56.104:20000</span><br><span class="line">$ curl 192.168.56.105:20000</span><br><span class="line"><span class="comment"># (Nginx welcome page should be displayed)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify the service</span></span><br><span class="line">$ kubectl get svc</span><br><span class="line"><span class="comment"># (copy the CLUSTER-IP for nginx-service here, e.g. 10.68.33.239) </span></span><br><span class="line"><span class="comment"># Use busybox image (sandbox within the cluster) for testing</span></span><br><span class="line">$ kubectl delete pod busybox</span><br><span class="line">$ kubectl run busybox --rm=<span class="literal">true</span> --image=busybox --restart=Never --tty -i</span><br><span class="line"><span class="comment"># In busybox container access nginx service with kube-proxy</span></span><br><span class="line">$ wget -qO - 10.68.33.239:8080</span><br><span class="line"><span class="comment"># One can also access the service directly through service name</span></span><br><span class="line">$ wget -qO - nginx-service:8080</span><br><span class="line"><span class="comment"># exit</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear-out the cluster after this demo session</span></span><br><span class="line">$ kubectl delete -f nginx-pod.yaml</span><br><span class="line">$ kubectl delete -f nginx-deployment.yaml</span><br><span class="line">$ kubectl delete -f nginx-service.yaml</span><br><span class="line">$ kubectl delete deploy kubernetes-bootcamp</span><br></pre></td></tr></table></figure>

<h2 id="Recommended-references"><a href="#Recommended-references" class="headerlink" title="Recommended references"></a>Recommended references</h2><p>Following YouTube links also provide some examples worth trying out:</p>
<p>[1] <a href="https://www.youtube.com/watch?v=K1HuOLzPSpU" target="_blank" rel="noopener">https://www.youtube.com/watch?v=K1HuOLzPSpU</a></p>
<p>[2] <a href="https://www.youtube.com/watch?v=yu3HlOXoEKk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=yu3HlOXoEKk</a></p>
<p>[3] <a href="https://www.youtube.com/watch?v=kvQ3VT_wH98" target="_blank" rel="noopener">https://www.youtube.com/watch?v=kvQ3VT_wH98</a></p>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s, demo</tag>
      </tags>
  </entry>
</search>
