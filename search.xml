<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k3s Dev (1) Environment Setup]]></title>
    <url>%2F2019%2F10%2F10%2Fk3s-dev-1-env-setup%2F</url>
    <content type="text"><![CDATA[This post shows in steps how to set up a dev environment for k3s project.Step 1. PreparationThe following list shows my local development environment setup: For code review &amp; build purpose: OS: Mac OS Docker: 18.06.1-ce JetBrains GoLand IDE golang: 1.12.7 For testing deploy purpose: VirtualBox Ubuntu 18.04 Server Alternatively, one could use EC2 instances as testing deploy environment, in which case may directly to go step 2. 1.1 VirtualBox SetupDownload Ubuntu 18.04 iso from official source and install it on VirtualBox. Configure the network interface as following: 12345# Check all available network adaptersifconfig -a# Then Use netplan to config all interfacessudo vi /etc/netplan/50-cloud-init.yaml Change the content accordingly (192.168.56.1 is my gateway setting, change it according to your own VirtualBox network setting): 123456789101112131415161718network: ethernets: enp0s3: addresses: [] dhcp4: true dhcp6: no optional: true nameservers: addresses: [8.8.8.8] enp0s8: addresses: [192.168.56.10/24] dhcp4: true dhcp6: no optional: true routes: - to: 192.168.56.1/24 via: 192.168.56.1 version: 2 Save and apply the changes: 1234567sudo netplan apply# Test connectivityping 192.168.56.101 # another virtual machine# Test installing k3scurl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR="/home/main/k3s" sh - 1.2 Golang EnvironmentFor Mac OS users, I recommend use homebrew to manage go environment. However, go has its own version manager called GVM, refer here for detailed information. To use earlier version of go, add following to your .bashrc file: 1234567891011121314151617181920212223# homebrew go version config goconfig() &#123; case "$1" in "1.12") brew switch go 1.12.7 export GOROOT=/usr/local/Cellar/go/1.12.7/libexec ;; "1.13") brew switch go 1.13 export GOROOT=/usr/local/Cellar/go/1.13/libexec ;; *) brew switch go 1.12.7 export GOROOT=/usr/local/Cellar/go/1.12.7/libexec ;; esac # Installed Go path export GOPATH=$HOME/go export PATH=$PATH:GOROOT/bing export PATH=$PATH:$(go env GOPATH)/bin&#125; To use simply pass the version selected. For example, to change to go version 12: 12# Switch to go version 12goconfig 1.12 1.3 Source Code DownloadWith go environment set up, now we go to its workspace to pull the source from github (I have already forked k3s project to my own github account, if that’s not the case for you, simply pull the source from rancher) 1234567891011121314cd /Users/yourname/go/src/github.com/yourgithubacct# Download my forked project to localgit clone --depth 1 https://github.com/ruby-/k3s.git# Add rancher remote sourcegit remote add rancher https://github.com/rancher/k3s.git# now there should be two remote sources: origin (my own) and rancher'sgit remote# fetch from rancher's master branch and merge to local (shouldn't be any change yet)git fetch ranchergit merge rancher/master Step 2. Build &amp; Test LaunchOpen the k3s folder in GoLand IDE, on Mac OS, press ⌘ + ⇧ + f to search in path for “k3s is up and running”, open the pkg/cli/server/server.go file and add following statement: 123456789101112131415161718192021func run(app *cli.Context, cfg *cmds.Server) error &#123; ... ctx := signals.SetupSignalHandler(context.Background()) certs, err := server.StartServer(ctx, &amp;serverConfig) if err != nil &#123; return err &#125; logrus.Info("k3s is up and running") // Add a new logging info here logrus.Info("==================&gt; Fog is just clouds that have fell down! &lt;==================") if notifySocket != "" &#123; os.Setenv("NOTIFY_SOCKET", notifySocket) systemd.SdNotify(true, "READY=1\n") &#125; ...&#125; Now open the terminal within GoLand IDE, then issue make command and wait for it’s pulling required docker images and build the project for us. After the building procedure, there should be hyperkube, k3s produced in the dist/artifacts/ folder. Simply copy k3s to your deploy environment for testing purpose (for me only need to copy to virtualbox ubuntu server that has been setup): 12345scp dist/artifacts/k3s main@192.168.56.10:~/k3s/# Start k3s server and verify the added logged infok3s server &gt; server.log 2&gt;&amp;1 &amp;vi server.log Note that: (1) Remember to chmod 777 k3s_install_path the install path for scp to be able to upload k3s executable; (2) k3s doesn’t support cross compile at current release, refer to issue#530 for detailed discussion regarding this; (3) There are two options for developers to build k3s from source. We are adapting docker method with rancher/dapper wrapper in this post. However for those who are working on Linux, may alternatively download the dependencies and directly build the source with go build commend described here; unfortunately this compile option is not available for Mac OS users at current release, refer to issue#273 for detailed discussion regarding this. (4) For Mac OS users, docker tends to accumulate its data file (store at /Users/yourname/Library/Containers/com.docker.docker/Data/vms/0/Docker.qcow2) which contains its self os image and downloaded containers. Build k3s project may consume ~10GB space on disk and it keeps growing. The way to get around this is to use docker built-in “reset” tab (Also note that docker system prune won’t save you from this pitfall). Step 3. Breakpoint &amp; debuggingIt is essential for us engineers to be able to set up breakpoint and debug the code in software development. In this section we’ll try to connect to remotely deployed k3s executable and use delve tool to navigate through the debugging process. 3.1 Install go and dlv in deploy envTo install and set up go environment and dlv debugger on Ubuntu 18.04 server VM we created earlier, simply follow these two guides: How To Install Go on Ubuntu 18.04 dlv - Installation on Linux To get familiar with dlv basic workflows and combine it with JetBrain GoLand IDE, I recommend to follow these practices: Using the Go Delve Debugger from the command line Debugging with GoLand – Getting Started Now let us go back to our k3s project, find and modify the following lines in scripts/build.sh: 1234567891011121314151617...VERSIONFLAGS=" -X $PKG/pkg/version.Version=$VERSION -X $PKG/pkg/version.GitCommit=$&#123;COMMIT:0:8&#125; -X $PKG/vendor/$PKG_CONTAINERD/version.Version=$VERSION_CONTAINERD -X $PKG/vendor/$PKG_CONTAINERD/version.Package=$PKG_RANCHER_CONTAINERD -X $PKG/vendor/$PKG_CRICTL/pkg/version.Version=$VERSION_CRICTL"# Comment out LDFLAGS (link) for debug: "-w"= wipe out debug info ;; "-s"= remove symbol table# LDFLAGS="# -w -s"LDFLAGS=""STATIC=" -extldflags '-static'"... Go to scripts/package-cli.sh file and change the following: 123456789101112...go generateLDFLAGS=" -X github.com/rancher/k3s/pkg/version.Version=$VERSION -X github.com/rancher/k3s/pkg/version.GitCommit=$&#123;COMMIT:0:8&#125; -w -s"STATIC="-extldflags '-static'"# CGO_ENABLED=0 go build -ldflags "$LDFLAGS $STATIC" -o $&#123;CMD_NAME&#125; ./cmd/k3s/main.go# Add gcflags to enable dlvCGO_ENABLED=0 go build -gcflags "all=-N -l" -ldflags "$STATIC" -o $&#123;CMD_NAME&#125; ./cmd/k3s/main.go Commit the file changes to our local git repo and rebuild the project, then upload the new k3s executable to our local VM: 123456789101112# Commit local changesgit add .git commit -m "enable dlv"# Rebuild projectmake# Upload to deployscp dist/artifacts/k3s main@192.168.56.10:~/k3s/# Launch with dlv: note `dlv exec xxx -- paras` means passing the parameters after "--" to our executable instead of dlvdlv --listen=:2345 --headless=true --api-version=2 --accept-multiclient exec /home/main/k3s/k3s -- --debug server Setup a new go remote debug run/debug configuration in GoLand IDE which looks like this: Toggle some breakpoint at main entry then click debug button to start debugging. Some of my questions: (1) Use the make method to build the whole project takes long time (3~5 min for me). I wonder if there’s some “quick” and “convenient” way to speedup this procedure, e.g. build only parts that have been changed? Please leave comments below if you know the answer to this. (2) The latest stable version of GoLand(2019.2) leave the channel open after the debugging procedure has been stopped within IDE. Simple kill the dlv by PID to get around this issue. (3) Run k3s command with generate a bunch of code in /var/lib/rancher/k3s/, these run time generated binaries and configurations contain most important components including k3s-server and k3s-agent. Details about these we’ll explore also in next my post. SummeryLearning k3s from source is important for us to contribute to its community. In my next post, I’ll try to investigate its scheduling mechanism and explain the related major workflow.]]></content>
      <categories>
        <category>edge computing</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k3s Basics]]></title>
    <url>%2F2019%2F09%2F29%2Fk3s-basics%2F</url>
    <content type="text"><![CDATA[This post explains my first impression with k3s.MotivationIn order to work on k3s project, one needs to learn to use it at first. This post summarize the main steps of deploying k3s in our local lab environment and also on AWS EC2. Local k3s cluster SetupThe basic hardware setup for this demo is as the same as the last post on k8s demo (3 VMs), feel free to apply the following methods with the latest released k3s on your own Raspbian nodes, it should work as we also tested it. However, the latest released version (0.9.1 at the time of this post) of k3s has some ca-related issues for an agent to join the master node (if a full ca-enabled Kubernetes cluster has been configured on these nodes), thus an earlier release version (0.2.0) was used for this section. Step 1: Stop the previously installed k8s related services (optional)On master node: 12345service kube-calico stopservice kube-scheduler stopservice kube-controller-manager stopservice kube-apiserver stopservice etcd stop &amp;&amp; rm -fr /var/lib/etcd/* On worker nodes: 123service kubelet stop &amp;&amp; rm -fr /var/lib/kubelet/*service kube-proxy stop &amp;&amp; rm -fr /var/lib/kube-proxy/*service kube-calico stop Here I simply turn k8s off to avoid potential conflicts between k8s and k3s deployment. However this doesn’t prevent the ca issue for latest k3s releases (&gt;v0.2.0) to work with previously installed k8s environment. Step 2: Deploy k3s on each nodeTo install k3s is quite simple, we will use the following commands install and start k3s on master and two worker machines separately. On master node: 123456789101112# Install k3s with rancher script# The script download the binaries, pull the images (containerd) and enable/start the k3s-related systemctl servicesmkdir k3s# With INSTALL_K3S_EXEC="--disable-agent" option, one may launch k3s server on the node without an agent (which may cause some issues with current release of k3s)curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR="/home/main/k3s" INSTALL_K3S_VERSION="v0.2.0" sh -# Verify the k3s system services are listening on their portsnetstat -nltp# Verify the k3s cluster statussystemctl status k3s On master node, one should see services and their port numbers: k3s : 6443/6444, 10251/10252 After installation, the k3s binary folder looks like this: 12345.├── crictl -&gt; k3s├── k3s├── k3s-killall.sh└── k3s-uninstall.sh With out the INSTALL_K3S_BIN_DIR option, k3s will be installed at /usr/local/bin Now, in order to join new workers to this master node, one needs first grab the token on that (master) node: 12# Example output: K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bbcat /var/lib/rancher/k3s/server/node-token On worker nodes: 1234567mkdir k3s# download &amp; active the k3s-agent servicecurl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR="/home/main/k3s" INSTALL_K3S_VERSION="v0.2.0" K3S_TOKEN="K10af00f60b1fa01b0a413e78922fd79efad2528bc4b0d19a357b5e2650d84252c5::node:f06ab2ff7068846d6b18b342f5f6a1bb" K3S_URL="https://192.168.56.103:6443" sh -# check the service statussystemctl status k3s-agent The installation of k3s agent on worker nodes looks like this: 12345.├── crictl -&gt; k3s├── k3s├── k3s-agent-uninstall.sh└── k3s-killall.sh On worker node, k3s has services and ports: k3s : 42323, containerd : 10010 Now on the master node, one should be able to verify the newly added cluster resources: 12345678910111213## Now verify the newly added worker in the clusterk3s kubectl get nodes## Do some deployment here...## Clean-up# Kill k3s services after inspection (on each node)k3s-killall.sh# Uninstall k3s on masterk3s-uninstall.sh# Uninstall k3s on worker nodesk3s-agent-uninstall.sh Install k3s manually on EC2 instancesThis section explains how to manually download k3s binaries from rancher’s official release and cluster-up. Here we use AWS EC2 service as following configuration: Instance OS Arch IP (internal) Instance Type vCPU Memory Node Role Ubuntu Server 18.04 LTS (HVM), SSD Volume Type amd64(x86_64) 172.31.46.70 t2.medium 2 4GiB master Amazon Linux 2 AMI (HVM), SSD Volume Type arm64(aarch64) 172.31.36.129 a1.medium 1 2GiB worker Remember to allow all traffic from anywhere in your security group setting. First, let us ssh to each running instance and prepare the k3s executable 12345678910111213141516# Change to your own pem key file and instance address:ssh -i ~/.ssh/your-key.pem ubuntu@ec2-x-x-x-x.region.compute.amazonaws.com# Prepare download foldermkdir k3scd k3s## Download the desired release version from: https://github.com/rancher/k3s/releases?after=v0.10.0-alpha1# On master (x86_64)wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s# On worker (arm)wget https://github.com/rancher/k3s/releases/download/v0.9.1/k3s-arm64mv k3s-arm64 k3s# Add exec modechmod +x k3s On master node: 12345# Start k3s server./k3s server &gt; server.log 2&gt;&amp;1 &amp;# Get token copy-and-paste the output to your worker:echo "export node_token=$(cat /var/lib/rancher/k3s/server/node-token)" On worker node: 12345# copy-and-paste token from master hereexport node_token=...# Start agent, pass server url and token./k3s agent --server https://172.31.46.70:6443 --token "$node_token" &gt;&amp; k3s-agent.log &amp; After a little while, check the cluster info with k3s kubectl command described in last section on the master node. Simply kill the PID to stop k3s or k3s-agent in the demo to shut down the cluster after inspection. ConclusionRanger k3s is much smaller and easier to deploy compared to Kubernetes, and it requires less effort and resources to set up. In next post, we will discuss how to set-up a development environment on k3s and dive deeper to learn k3s from its source code.]]></content>
      <categories>
        <category>edge computing</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s Basics]]></title>
    <url>%2F2019%2F09%2F17%2Fk8s-basics%2F</url>
    <content type="text"><![CDATA[This post demonstrate some Kubernetes basic commends. Server OverviewWe have setup 3 virtual machines，each has 1 cpu and 1GB memory. Details： Server OS IP Address Node Type CPU Memory Hostname ubuntu16.04 192.168.56.103 master 1 1G server01 ubuntu16.04 192.168.56.104 slave1 1 1G server02 ubuntu16.04 192.168.56.105 slave2 1 1G server03 To follow this demo, root privilege is required, ask system admin (Todd) for root access. Startup all nodesLogin to the system then start virtualBox and each virtual machine. Launch VirtualBox1234# get root privilege$ sudo -s# Then start master, slave1 and slave2 virtual machines $ virtualbox Login to each node12345$ Login as: main# passwd: 000000# And get root priviledge on each node$ sudo -s Check system information on each node123456# check ip address$ ifconfig# check running containers$ docker ps# check services &amp; ports$ netstat -nltp On master node, one should see services and their port numbers: kube-apiserver : 6443/8080, etcd : 2379/2380, kube-scheduler : 10251, kube-controller : 10252, calico-felix : 9099 On worker nodes, one should see services and their port numbers: kubelet : 4194/10248/10250/10255, kube-proxy:10249/10256, calico-felix : 9099 Commonly used commandsUse calico to check the network status on each node1$ calicoctl node status At each node, it should be able to see the other two nodes’ ip addresses in the cluster. Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)12345678910# check server/client version$ kubectl version# get workers$ kubectl get node# get pods$ kubectl get pods# get deployment$ kubectl get deploy# get services$ kubectl get svc More kubectl commands123456789101112131415161718192021222324252627$ kubectl run kubernetes-bootcamp --image=jocatalin/kubernetes-bootcamp:v1 --port=8080# check deploy/pods again$ kubectl get deploy$ kubectl get pods# i.e. NAME: kubernetes-bootcamp-6b7849c495-p7dsw# Then check log of the pod$ kubectl logs kubernetes-bootcamp-6b7849c495-p7dsw -f# (ctrl-c out the following log)# describe pod$ kubectl describe pods kubernetes-bootcamp-6b7849c495-p7dsw# (Find the Mounts:/var/run/secrets/kubernetes.io/serviceaccount in the description)# Enter the running pod and verify the above path$ kubectl exec -it kubernetes-bootcamp-6b7849c495-p7dsw bash# Find out the certificate files in that path$ ls -l /var/run/secrets/kubernetes.io/serviceaccount # Exit the pod$ exit# These ca files are actually associate with the ca account, check that info by:$ kubectl get sa -o yaml# can also output other pattern, e.g. json# check the "secret"$ kubectl get secrets -o yaml# (The content of this secret has 3 sections that are mounted as three files in each pod as we see in above) The “secret” are mounted to each created pod as files located in /var/run/secrets/… so that each pod can connect with api-server with https requests. Use ‘apply’ or ‘create’ with yaml filesKubectl ‘apply’ command is similar to ‘create’ command, but has rich properties. 1234567891011121314151617181920212223242526272829303132333435363738394041$ cd services# Create a nginx pod and verify$ kubectl apply -f nginx-pod.yaml$ kubectl describe pod nginx# (version here used is 1.7.9)# vi nginx-pod.yaml and change image:nginx:1.7.9 -&gt; image:nginx:1.13, re-apply the yaml file$ kubectl apply -f nginx-pod.yaml# (version here is now 1.13)# Another way to change version of running image is to use 'set' command$ kubectl set image pods nginx nginx=nginx:1.7.9# (reset the version to 1.7.9)# 'apply' command can be also used to create other resources$ kubectl apply -f nginx-deployment.yaml$ kubectl apply -f nginx-service.yaml# Check the service is runing$ curl 192.168.56.104:20000$ curl 192.168.56.105:20000# (Nginx welcome page should be displayed)# Verify the service$ kubectl get svc# (copy the CLUSTER-IP for nginx-service here, e.g. 10.68.33.239) # Use busybox image (sandbox within the cluster) for testing$ kubectl delete pod busybox$ kubectl run busybox --rm=true --image=busybox --restart=Never --tty -i# In busybox container access nginx service with kube-proxy$ wget -qO - 10.68.33.239:8080# One can also access the service directly through service name$ wget -qO - nginx-service:8080# exit$ exit# clear-out the cluster after this demo session$ kubectl delete -f nginx-pod.yaml$ kubectl delete -f nginx-deployment.yaml$ kubectl delete -f nginx-service.yaml$ kubectl delete deploy kubernetes-bootcamp Recommended referencesFollowing YouTube links also provide some examples worth trying out: [1] https://www.youtube.com/watch?v=K1HuOLzPSpU [2] https://www.youtube.com/watch?v=yu3HlOXoEKk [3] https://www.youtube.com/watch?v=kvQ3VT_wH98]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s, demo</tag>
      </tags>
  </entry>
</search>
