<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k3s-basics]]></title>
    <url>%2Fruby-.github.io%2F2019%2F09%2F29%2Fk3s-basics%2F</url>
    <content type="text"><![CDATA[My First Impression on K3SMotivationIn order to work on k3s project, one needs to learn to use it at first. This post summarize the main steps of deploying k3s on my local environment, and how to setup the development environment. Running Environment SetupThe basic hardware setup is as the same as my last post on k8s demo (3 VMs). Step 1: Stop the previously installed k8s related services (optional)On master node: 12345service kube-calico stopservice kube-scheduler stopservice kube-controller-manager stopservice kube-apiserver stopservice etcd stop &amp;&amp; rm -fr /var/lib/etcd/* On worker nodes: 123service kubelet stop &amp;&amp; rm -fr /var/lib/kubelet/*service kube-proxy stop &amp;&amp; rm -fr /var/lib/kube-proxy/*service kube-calico stop Here I simply turn k8s off to avoid potential conflicts between k8s and k3s deployment. However this is just my assumption, they may perfectly work together after all. Step 2: Deploy k3s on each nodeTo install k3s is quite simple, we will use the following commands install and start k3s on master and two worker machines separately. On master node: 123456789101112# Install k3s with rancher script# The script download the binaries, pull the images (containerd) and enable/start the k3s-related systemctl servicesmkdir k3s# With INSTALL_K3S_EXEC="--disable-agent" option, one may launch k3s server on the node without an agent (which may cause some issues with current release of k3s)curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR="/home/main/k3s" INSTALL_K3S_VERSION="v0.1.0" sh -# Verify the k3s system services are listening on their portsnetstat -nltp# Verify the k3s cluster statussystemctl status k3s On master node, one should see services and their port numbers: k3s : 6443/6444, 10251/10252 After installation, the k3s binary folder looks like this: 12345.├── crictl -&gt; k3s├── k3s├── k3s-killall.sh└── k3s-uninstall.sh With out the INSTALL_K3S_BIN_DIR option, k3s will be installed at /usr/local/bin Now, in order to join new workers to this master node, one needs first grab the token on that (master) node: 1cat /var/lib/rancher/k3s/server/node-token On worker nodes: 1234567mkdir k3s# download &amp; active the k3s-agent servicecurl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR="/home/main/k3s" INSTALL_K3S_VERSION="v0.1.0" K3S_TOKEN="K1069e9389280a39cfeefb5b17cc43bee8c1a357946fb40c0b5734d6226be4c2eb6::node:2c22cc54a2816ea6d01da73ab56e26b3" K3S_URL="https://192.168.56.103:6443" sh -# check the service statussystemctl status k3s-agent The installation of k3s agent on worker nodes looks like this: 12345.├── crictl -&gt; k3s├── k3s├── k3s-agent-uninstall.sh└── k3s-killall.sh On worker node, k3s has services and ports: k3s : 42323, containerd : 10010 Now on the master node, one should be able to verify the newly added cluster resources: 123456789k3s kubectl get nodes# Kill k3s services after inspection (on each node)k3s-killall.sh# Uninstall k3s on masterk3s-uninstall.sh# Uninstall k3s on worker nodesk3s-agent-uninstall.sh Dev Environment SetupTo be continued… ConclusionRanger k3s is much smaller and easier to deploy compared to Kubernetes. The new version (0.8.3 due to the time of this post) of k3s has some problems with the joining of an agent to the master node, thus the first release version (0.1.0) was used in this demo, later I’ll test to see a newer version that doesn’t have the problem and switch my dev environment onto that branch. In summery, k3s is lightweight and thus better fit for our research on edge computing in favor of time compared to k8s.]]></content>
      <categories>
        <category>edge</category>
      </categories>
      <tags>
        <tag>k3s, k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-basics]]></title>
    <url>%2Fruby-.github.io%2F2019%2F09%2F17%2Fk8s-basics%2F</url>
    <content type="text"><![CDATA[Demo of Kubernetes BasicsServer OverviewWe have setup 3 virtual machines，each has 1 cpu and 1GB memory. Details： Server OS IP Address Node Type CPU Memory Hostname ubuntu16.04 192.168.56.103 master 1 1G server01 ubuntu16.04 192.168.56.104 slave1 1 1G server02 ubuntu16.04 192.168.56.105 slave2 1 1G server03 To follow this demo, root privilege is required, ask system admin (Todd) for root access. Startup all nodesLogin to the system then start virtualBox and each virtual machine. Launch VirtualBox1234# get root privilege$ sudo -s# Then start master, slave1 and slave2 virtual machines $ virtualbox Login to each node12345$ Login as: main# passwd: 000000# And get root priviledge on each node$ sudo -s Check system information on each node123456# check ip address$ ifconfig# check running containers$ docker ps# check services &amp; ports$ netstat -nltp On master node, one should see services and their port numbers: kube-apiserver : 6443/8080, etcd : 2379/2380, kube-scheduler : 10251, kube-controller : 10252, calico-felix : 9099 On worker nodes, one should see services and their port numbers: kubelet : 4194/10248/10250/10255, kube-proxy:10249/10256, calico-felix : 9099 Commonly used commandsUse calico to check the network status on each node1$ calicoctl node status At each node, it should be able to see the other two nodes’ ip addresses in the cluster. Use kubectl on master node to verify the cluster resources (deloyment, nodes, pods, services, etc.)12345678910# check server/client version$ kubectl version# get workers$ kubectl get node# get pods$ kubectl get pods# get deployment$ kubectl get deploy# get services$ kubectl get svc More kubectl commands123456789101112131415161718192021222324252627$ kubectl run kubernetes-bootcamp --image=jocatalin/kubernetes-bootcamp:v1 --port=8080# check deploy/pods again$ kubectl get deploy$ kubectl get pods# i.e. NAME: kubernetes-bootcamp-6b7849c495-p7dsw# Then check log of the pod$ kubectl logs kubernetes-bootcamp-6b7849c495-p7dsw -f# (ctrl-c out the following log)# describe pod$ kubectl describe pods kubernetes-bootcamp-6b7849c495-p7dsw# (Find the Mounts:/var/run/secrets/kubernetes.io/serviceaccount in the description)# Enter the running pod and verify the above path$ kubectl exec -it kubernetes-bootcamp-6b7849c495-p7dsw bash# Find out the certificate files in that path$ ls -l /var/run/secrets/kubernetes.io/serviceaccount # Exit the pod$ exit# These ca files are actually associate with the ca account, check that info by:$ kubectl get sa -o yaml# can also output other pattern, e.g. json# check the "secret"$ kubectl get secrets -o yaml# (The content of this secret has 3 sections that are mounted as three files in each pod as we see in above) The “secret” are mounted to each created pod as files located in /var/run/secrets/… so that each pod can connect with api-server with https requests. Use ‘apply’ or ‘create’ with yaml filesKubectl ‘apply’ command is similar to ‘create’ command, but has rich properties. 1234567891011121314151617181920212223242526272829303132333435363738394041$ cd services# Create a nginx pod and verify$ kubectl apply -f nginx-pod.yaml$ kubectl describe pod nginx# (version here used is 1.7.9)# vi nginx-pod.yaml and change image:nginx:1.7.9 -&gt; image:nginx:1.13, re-apply the yaml file$ kubectl apply -f nginx-pod.yaml# (version here is now 1.13)# Another way to change version of running image is to use 'set' command$ kubectl set image pods nginx nginx=nginx:1.7.9# (reset the version to 1.7.9)# 'apply' command can be also used to create other resources$ kubectl apply -f nginx-deployment.yaml$ kubectl apply -f nginx-service.yaml# Check the service is runing$ curl 192.168.56.104:20000$ curl 192.168.56.105:20000# (Nginx welcome page should be displayed)# Verify the service$ kubectl get svc# (copy the CLUSTER-IP for nginx-service here, e.g. 10.68.33.239) # Use busybox image (sandbox within the cluster) for testing$ kubectl delete pod busybox$ kubectl run busybox --rm=true --image=busybox --restart=Never --tty -i# In busybox container access nginx service with kube-proxy$ wget -qO - 10.68.33.239:8080# One can also access the service directly through service name$ wget -qO - nginx-service:8080# exit$ exit# clear-out the cluster after this demo session$ kubectl delete -f nginx-pod.yaml$ kubectl delete -f nginx-deployment.yaml$ kubectl delete -f nginx-service.yaml$ kubectl delete deploy kubernetes-bootcamp Recommended referencesFollowing YouTube links also provide some examples worth trying out: [1] https://www.youtube.com/watch?v=K1HuOLzPSpU [2] https://www.youtube.com/watch?v=yu3HlOXoEKk [3] https://www.youtube.com/watch?v=kvQ3VT_wH98]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s, demo</tag>
      </tags>
  </entry>
</search>
